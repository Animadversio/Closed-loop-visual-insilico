import time
import numpy as np
from math import sqrt
import torch
from sklearn.random_projection import johnson_lindenstrauss_min_dim, SparseRandomProjection
from sklearn.random_projection import _sparse_random_matrix
# from neural_regress.SRP_torch_lib import _sparse_random_matrix_torch

# --- Torch version  ---
def _sparse_random_matrix_torch(n_components, n_features, density="auto", random_state=None, device="cpu", use_multinomial=False):
    """
    Torch version of the sparse random matrix generator.
    Returns a torch tensor: dense if density==1, otherwise a sparse_coo_tensor.
    CAVEAT: due to different generator in torch, numpy and across different devices, 
        the actual random matrix generated by torch is different from the numpy version. 
    TODO: find a way to make the two versions more consistent.
    """
    if density == "auto":
        density = 1 / sqrt(n_features)
    if not (0 < density <= 1):
        raise ValueError("density must be in (0, 1].")
    
    # Setup a torch generator to have reproducible results
    gen = torch.Generator(device=device if device != "mps" else "cpu")
    if random_state is not None:
        gen.manual_seed(random_state)
    
    if density == 1:
        # Dense: each entry is randomly ±1
        components = torch.randint(0, 2, (n_components, n_features), 
                                 generator=gen, device=device)
        components = components * 2 - 1
        return components / sqrt(n_components)
    else:
        indices_list = []
        values_list = []
        # For each row, sample the number of nonzeros and choose indices without replacement
        for i in range(n_components):
            # Draw number of nonzero entries via a Binomial distribution.
            # We use torch.distributions.Binomial to sample
            binom = torch.distributions.Binomial(total_count=n_features, probs=density)
            n_nonzero = int(binom.sample().item())
            feature_weights = torch.ones(n_features, device=device) # / n_features
            if n_nonzero > 0:
                # Choose without replacement: use randperm and take the first n_nonzero entries
                # row_indices = torch.randperm(n_features, generator=gen, device=device)[:n_nonzero]
                if use_multinomial:
                    row_indices = torch.multinomial(feature_weights, n_nonzero, replacement=False)
                else:
                    row_indices = torch.randperm(n_features, generator=gen, device=device)[:n_nonzero]
                # Sorting the indices (optional)
                row_indices, _ = torch.sort(row_indices)
                # Append the row index repeated for each nonzero
                row_ids = torch.full((n_nonzero,), i, dtype=torch.long, device=device)
                indices_list.append(torch.stack([row_ids, row_indices], dim=0))
                # Random ±1 values
                values = torch.randint(0, 2, (n_nonzero,), generator=gen, device=device) * 2 - 1
                values_list.append(values.to(torch.float32))
        
        if indices_list:
            indices = torch.cat(indices_list, dim=1)
            values = torch.cat(values_list)
        else:
            indices = torch.empty((2, 0), dtype=torch.long, device=device)
            values = torch.tensor([], dtype=torch.float32, device=device)
        
        # Scale factor as in the numpy version
        scale = sqrt(1 / density) / sqrt(n_components)
        values = values * scale  # Scale the values before creating the tensor
        
        # Create the sparse tensor with scaled values
        sparse_tensor = torch.sparse_coo_tensor(indices, values, 
                                              size=(n_components, n_features),
                                              device=device)
        return sparse_tensor


def benchmark_sparse_matrix_generation_and_matmul(n_samples = 1000,
                                        n_components = "auto",
                                        n_features = 500000,
                                        density = "auto",
                                        random_state = 42):
    print(f"Benchmarking sparse matrix generation and matmul with n_samples={n_samples}, n_features={n_features}, n_components={n_components}, density={density}, random_state={random_state}")
    if n_components == "auto":
        n_components = johnson_lindenstrauss_min_dim(n_samples, eps=0.1)
    # Time numpy version
    t0 = time.time()
    np_matrix = _sparse_random_matrix(n_components, n_features, density=density, random_state=random_state)
    np_time = time.time() - t0
    print(f"Numpy matrix generation time: {np_time:.4f} seconds")
    # print the size and non-zero ratio of the numpy matrix
    print(f"Numpy matrix size: {np_matrix.shape}")
    print(f"Numpy matrix non-zero ratio: {np_matrix.nnz/np_matrix.shape[0]/np_matrix.shape[1]:.4f}")
    # Time torch version 
    t0 = time.time()
    torch_matrix = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device="cpu")
    torch_time = time.time() - t0
    print(f"Torch matrix on CPU generation time: {torch_time:.4f} seconds")
    # print the size and non-zero ratio of the torch matrix
    print(f"Torch matrix size: {torch_matrix.shape}")
    print(f"Torch matrix non-zero ratio: {torch_matrix._nnz()/torch_matrix.shape[0]/torch_matrix.shape[1]:.4f}")
    # Time torch version on GPU
    t0 = time.time()
    torch_matrix_cuda = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device="cuda")
    torch_cuda_time = time.time() - t0
    print(f"Torch matrix on GPU generation time: {torch_cuda_time:.4f} seconds")
    # print the size and non-zero ratio of the torch matrix
    print(f"Torch matrix size: {torch_matrix_cuda.shape}")
    print(f"Torch matrix non-zero ratio: {torch_matrix_cuda._nnz()/torch_matrix_cuda.shape[0]/torch_matrix_cuda.shape[1]:.4f}")
    print(f"Speedup factor: {np_time/torch_cuda_time:.2f}x")
    
    # benchmark the time it takes to matmul a dense random matrix
    featmat = torch.randn(n_samples, n_features,)
    # benchmark the time it takes to matmul with numpy
    t0 = time.time()
    featmat.numpy() @ np_matrix.T
    np_time = time.time() - t0
    print(f"Numpy matrix matmul time: {np_time:.4f} seconds")
    
    # benchmark the time it takes to matmul with torch matrix on CPU
    t0 = time.time()
    featmat @ torch_matrix.T
    torch_cpu_time = time.time() - t0
    print(f"Torch matrix on CPU matmul time: {torch_cpu_time:.4f} seconds")
    t0 = time.time()
    torch.sparse.mm(featmat, torch_matrix.T)
    torch_cpu_time = time.time() - t0
    print(f"Torch matrix on CPU sparse matmul time: {torch_cpu_time:.4f} seconds")
    
    # benchmark the time it takes to matmul with torch matrix on GPU
    t0 = time.time()
    featmat.to("cuda") @ torch_matrix_cuda.T
    torch_cuda_time = time.time() - t0
    print(f"Torch matrix on GPU matmul time: {torch_cuda_time:.4f} seconds")
    t0 = time.time()
    torch.sparse.mm(featmat.to("cuda"), torch_matrix_cuda.T)
    torch_cuda_time = time.time() - t0
    print(f"Torch matrix on GPU sparse matmul time: {torch_cuda_time:.4f} seconds")
    print(f"Speedup factor: {np_time/torch_cuda_time:.2f}x")
    return np_matrix, torch_matrix, torch_matrix_cuda

# build a pytorch version of the sklearn SparseRandomProjection
import warnings
import scipy.sparse as sp
from sklearn.exceptions import DataDimensionalityWarning
def torch_to_sklearn_projection(torch_sparse, n_components, ):
    # Move to CPU and ensure the tensor is coalesced
    torch_sparse = torch_sparse.cpu().coalesce()
    # Extract indices and values from the torch sparse tensor.
    indices = torch_sparse.indices().numpy()
    values = torch_sparse.values().numpy()
    shape = torch_sparse.size()  # Expected shape: (n_components, n_features)
    # Create a scipy sparse matrix (using COO and converting to CSR for efficiency)
    scipy_random_matrix = sp.coo_matrix((values, (indices[0], indices[1])), shape=shape).tocsr()
    # Initialize a SparseRandomProjection instance.
    srp = SparseRandomProjection(n_components=n_components, dense_output=True)
    # Assign the random projection matrix to the sklearn object.
    srp.components_ = scipy_random_matrix
    return srp


def SparseRandomProjection_fit_transform_torch(X, n_components="auto", eps=0.1,
                                               density="auto", random_state=None, 
                                               device="cuda", use_multinomial=False):
    """
    Fit the sparse random matrix and transform the data.
    Using torch cuda operations to speed up the computation. Note it could be even slower if using CPU.
    """
    n_samples, n_features = X.shape
    # Determine the number of components
    if n_components == "auto":
        n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps)
        if n_components <= 0:
            raise ValueError(
                "eps=%f and n_samples=%d lead to a target dimension of "
                "%d which is invalid" % (eps, n_samples, n_components)
            )

        elif n_components > n_features:
            raise ValueError(
                "eps=%f and n_samples=%d lead to a target dimension of "
                "%d which is larger than the original space with "
                "n_features=%d"
                % (eps, n_samples, n_components, n_features)
            )
    elif isinstance(n_components, int):
        if n_components > n_features:
            warnings.warn(
                "The number of components is higher than the number of"
                " features: n_features < n_components (%s < %s)."
                "The dimensionality of the problem will not be reduced."
                % (n_features, n_components),
                DataDimensionalityWarning,
            )
    # construct the projection matrix, on device (potentially GPU)
    projection_matrix = _sparse_random_matrix_torch(n_components, n_features, density=density, 
                                                    random_state=random_state, 
                                                    device=device, use_multinomial=use_multinomial)
    # transform the data, using sparse matrix multiplication with torch
    featmat = torch.sparse.mm(X.float().to(device), projection_matrix.T).to("cpu") # note some x are half precision
    srp_sklearn = torch_to_sklearn_projection(projection_matrix, n_components)
    return featmat, srp_sklearn


def test_torch_SparseRandomProjection(n_samples=1000, 
                                 n_features=50000, 
                                 n_components="auto", 
                                 eps=0.1, random_state = 42):
    X = torch.randn(n_samples, n_features)
    featmat_torch, srp_sklearn = SparseRandomProjection_fit_transform_torch(X, n_components, eps=0.1, random_state=random_state,
                                                                device="cuda", )
    # test the time it takes to transform the data
    t0 = time.time()
    featmat_sklearn = srp_sklearn.transform(X.numpy())
    sklearn_time = time.time() - t0
    featmat_sklearn_torch = torch.from_numpy(featmat_sklearn)
    # test they are the same
    max_abs_diff = torch.max(torch.abs(featmat_torch - featmat_sklearn_torch))
    print(f"max absolute difference: {max_abs_diff:.2e}")
    assert torch.allclose(featmat_torch, featmat_sklearn_torch, atol=1e-5), f"The result from sklearn and torch are different, max absolute difference: {max_abs_diff:.6f}"
    print(f"The result from sklearn and torch are the same")
    return 


def benchmark_SparseRandomProjection(n_samples=1000, 
                                 n_features=50000, 
                                 n_components="auto", 
                                 eps=0.1, random_state = 42):
    print(f"Benchmarking SparseRandomProjection with n_samples={n_samples}, n_features={n_features}, n_components={n_components}, eps={eps}, random_state={random_state}")
    X = torch.randn(n_samples, n_features)
    X_np = X.numpy()

    srp_sklearn = SparseRandomProjection(n_components=n_components, dense_output=True, 
                            eps=eps, random_state=random_state)
    t0 = time.time()
    featmat_sklearn = srp_sklearn.fit_transform(X_np)
    sklearn_time = time.time() - t0
    print(f"Sklearn time: {sklearn_time:.4f} seconds")
    
    t0 = time.time()
    featmat_torch, srp_sklearn_translated = SparseRandomProjection_fit_transform_torch(X, n_components=n_components, 
                            eps=eps, random_state=random_state, device="cuda", )
    torch_time = time.time() - t0
    print(f"Torch time: {torch_time:.4f} seconds")
    print(f"Speedup factor: {sklearn_time/torch_time:.2f}x")
    return featmat_torch, featmat_sklearn


if __name__ == "__main__":
    benchmark_SparseRandomProjection(n_samples = 1000,
                                n_features = 50000,
                                n_components = "auto",
                                eps = 0.1,
                                random_state = 42)
    
    benchmark_SparseRandomProjection(n_samples = 1000,
                                n_features = 500000,
                                n_components = "auto",
                                eps = 0.1,
                                random_state = 42)
    print("--------------------------------")
    benchmark_sparse_matrix_generation_and_matmul(n_samples = 1000,
                                n_components = "auto",
                                n_features = 50000,
                                density = "auto",
                                random_state = 42)
    
    benchmark_sparse_matrix_generation_and_matmul(n_samples = 1000,
                                n_components = "auto",
                                n_features = 500000,
                                density = "auto",
                                random_state = 42)
    