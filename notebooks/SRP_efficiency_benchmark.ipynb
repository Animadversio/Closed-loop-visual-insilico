{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.random_projection import SparseRandomProjection, _sparse_random_matrix, johnson_lindenstrauss_min_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_matrix = _sparse_random_matrix(n_components=1000, n_features=500000, density=\"auto\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_ = johnson_lindenstrauss_min_dim(n_samples=1000, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_ = johnson_lindenstrauss_min_dim(n_samples=1000, eps=0.1) # this step is efficient. \n",
    "projection_matrix = _sparse_random_matrix(n_components=n_components_, n_features=500000, density=\"auto\", random_state=42) # this step is not efficient! in the old implementation, it uses a dense matrix to generate the projection matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 4185511 stored elements and shape (5920, 500000)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "# --- Original numpy/scipy version for reference ---\n",
    "def _check_input_size(n_components, n_features):\n",
    "    if n_components <= 0 or n_features <= 0:\n",
    "        raise ValueError(\"n_components and n_features must be > 0.\")\n",
    "\n",
    "def _check_density(density, n_features):\n",
    "    if density == \"auto\":\n",
    "        density = 1 / sqrt(n_features)\n",
    "    if not (0 < density <= 1):\n",
    "        raise ValueError(\"density must be in (0, 1].\")\n",
    "    return density\n",
    "\n",
    "def sample_without_replacement(n, k, random_state):\n",
    "    # a simple implementation using np.random.choice with replace=False\n",
    "    return np.random.choice(n, size=k, replace=False)\n",
    "\n",
    "def _sparse_random_matrix(n_components, n_features, density=\"auto\", random_state=None):\n",
    "    \"\"\"Original numpy/scipy version.\"\"\"\n",
    "    _check_input_size(n_components, n_features)\n",
    "    density = _check_density(density, n_features)\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    if density == 1:\n",
    "        components = rng.binomial(1, 0.5, (n_components, n_features)) * 2 - 1\n",
    "        return 1 / np.sqrt(n_components) * components\n",
    "    else:\n",
    "        indices = []\n",
    "        offset = 0\n",
    "        indptr = [offset]\n",
    "        for _ in range(n_components):\n",
    "            n_nonzero_i = rng.binomial(n_features, density)\n",
    "            indices_i = sample_without_replacement(n_features, n_nonzero_i, random_state=rng)\n",
    "            indices.append(indices_i)\n",
    "            offset += n_nonzero_i\n",
    "            indptr.append(offset)\n",
    "\n",
    "        indices = np.concatenate(indices) if indices else np.array([], dtype=int)\n",
    "        data = rng.binomial(1, 0.5, size=np.size(indices)) * 2 - 1\n",
    "\n",
    "        components = sp.csr_matrix(\n",
    "            (data, indices, indptr), shape=(n_components, n_features)\n",
    "        )\n",
    "\n",
    "        return np.sqrt(1 / density) / np.sqrt(n_components) * components\n",
    "\n",
    "# --- Torch version  ---\n",
    "def _sparse_random_matrix_torch(n_components, n_features, density=\"auto\", random_state=None, device=\"cpu\", use_multinomial=False):\n",
    "    \"\"\"\n",
    "    Torch version of the sparse random matrix generator.\n",
    "    Returns a torch tensor: dense if density==1, otherwise a sparse_coo_tensor.\n",
    "    \"\"\"\n",
    "    if density == \"auto\":\n",
    "        density = 1 / sqrt(n_features)\n",
    "    if not (0 < density <= 1):\n",
    "        raise ValueError(\"density must be in (0, 1].\")\n",
    "    \n",
    "    # Setup a torch generator to have reproducible results\n",
    "    gen = torch.Generator(device=device if device != \"mps\" else \"cpu\")\n",
    "    if random_state is not None:\n",
    "        gen.manual_seed(random_state)\n",
    "    \n",
    "    if density == 1:\n",
    "        # Dense: each entry is randomly ±1\n",
    "        components = torch.randint(0, 2, (n_components, n_features), \n",
    "                                 generator=gen, device=device)\n",
    "        components = components * 2 - 1\n",
    "        return components / sqrt(n_components)\n",
    "    else:\n",
    "        indices_list = []\n",
    "        values_list = []\n",
    "        # For each row, sample the number of nonzeros and choose indices without replacement\n",
    "        for i in range(n_components):\n",
    "            # Draw number of nonzero entries via a Binomial distribution.\n",
    "            # We use torch.distributions.Binomial to sample\n",
    "            binom = torch.distributions.Binomial(total_count=n_features, probs=density)\n",
    "            n_nonzero = int(binom.sample().item())\n",
    "            feature_weights = torch.ones(n_features, device=device) # / n_features\n",
    "            if n_nonzero > 0:\n",
    "                # Choose without replacement: use randperm and take the first n_nonzero entries\n",
    "                # row_indices = torch.randperm(n_features, generator=gen, device=device)[:n_nonzero]\n",
    "                if use_multinomial:\n",
    "                    row_indices = torch.multinomial(feature_weights, n_nonzero, replacement=False)\n",
    "                else:\n",
    "                    row_indices = torch.randperm(n_features, generator=gen, device=device)[:n_nonzero]\n",
    "                # Sorting the indices (optional)\n",
    "                row_indices, _ = torch.sort(row_indices)\n",
    "                # Append the row index repeated for each nonzero\n",
    "                row_ids = torch.full((n_nonzero,), i, dtype=torch.long, device=device)\n",
    "                indices_list.append(torch.stack([row_ids, row_indices], dim=0))\n",
    "                # Random ±1 values\n",
    "                values = torch.randint(0, 2, (n_nonzero,), generator=gen, device=device) * 2 - 1\n",
    "                values_list.append(values.to(torch.float32))\n",
    "        \n",
    "        if indices_list:\n",
    "            indices = torch.cat(indices_list, dim=1)\n",
    "            values = torch.cat(values_list)\n",
    "        else:\n",
    "            indices = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "            values = torch.tensor([], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Scale factor as in the numpy version\n",
    "        scale = sqrt(1 / density) / sqrt(n_components)\n",
    "        values = values * scale  # Scale the values before creating the tensor\n",
    "        \n",
    "        # Create the sparse tensor with scaled values\n",
    "        sparse_tensor = torch.sparse_coo_tensor(indices, values, \n",
    "                                              size=(n_components, n_features),\n",
    "                                              device=device)\n",
    "        return sparse_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of random index selection through multinomial vs permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch matrix on GPU generation time: 2.1220 seconds\n",
      "Torch matrix on GPU generation time: 1.6251 seconds with use_multinomial=False\n",
      "Torch matrix on CPU generation time: 16.0217 seconds\n",
      "Torch matrix on CPU generation time: 19.9218 seconds with use_multinomial=False\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_components = 5000\n",
    "n_features = 500000\n",
    "density = \"auto\"\n",
    "random_state = 42\n",
    "# torch_matrix = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device=\"cpu\")\n",
    "t0 = time.time()\n",
    "torch_matrix_cuda = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device=\"cuda\", use_multinomial=True)\n",
    "torch_cuda_time = time.time() - t0\n",
    "print(f\"Torch matrix on GPU generation time: {torch_cuda_time:.4f} seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "torch_matrix_cuda = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device=\"cuda\", use_multinomial=False)\n",
    "torch_cuda_time = time.time() - t0\n",
    "print(f\"Torch matrix on GPU generation time: {torch_cuda_time:.4f} seconds with use_multinomial=False\")\n",
    "\n",
    "t0 = time.time()\n",
    "torch_matrix_cuda = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device=\"cpu\", use_multinomial=True)\n",
    "torch_cuda_time = time.time() - t0\n",
    "print(f\"Torch matrix on CPU generation time: {torch_cuda_time:.4f} seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "torch_matrix_cuda = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device=\"cpu\", use_multinomial=False)\n",
    "torch_cuda_time = time.time() - t0\n",
    "print(f\"Torch matrix on CPU generation time: {torch_cuda_time:.4f} seconds with use_multinomial=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_sparse_random_projection(n_samples = 1000,\n",
    "                                        n_components = \"auto\",\n",
    "                                        n_features = 500000,\n",
    "                                        density = \"auto\",\n",
    "                                        random_state = 42):\n",
    "    if n_components == \"auto\":\n",
    "        n_components = johnson_lindenstrauss_min_dim(n_samples, eps=0.1)\n",
    "    # Time numpy version\n",
    "    t0 = time.time()\n",
    "    np_matrix = _sparse_random_matrix(n_components, n_features, density=density, random_state=random_state)\n",
    "    np_time = time.time() - t0\n",
    "    print(f\"Numpy matrix generation time: {np_time:.4f} seconds\")\n",
    "    # print the size and non-zero ratio of the numpy matrix\n",
    "    print(f\"Numpy matrix size: {np_matrix.shape}\")\n",
    "    print(f\"Numpy matrix non-zero ratio: {np_matrix.nnz/np_matrix.shape[0]/np_matrix.shape[1]:.4f}\")\n",
    "    # Time torch version \n",
    "    t0 = time.time()\n",
    "    torch_matrix = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device=\"cpu\")\n",
    "    torch_time = time.time() - t0\n",
    "    print(f\"Torch matrix on CPU generation time: {torch_time:.4f} seconds\")\n",
    "    # print the size and non-zero ratio of the torch matrix\n",
    "    print(f\"Torch matrix size: {torch_matrix.shape}\")\n",
    "    print(f\"Torch matrix non-zero ratio: {torch_matrix._nnz()/torch_matrix.shape[0]/torch_matrix.shape[1]:.4f}\")\n",
    "    # Time torch version on GPU\n",
    "    t0 = time.time()\n",
    "    torch_matrix_cuda = _sparse_random_matrix_torch(n_components, n_features, density=density, random_state=random_state, device=\"cuda\")\n",
    "    torch_cuda_time = time.time() - t0\n",
    "    print(f\"Torch matrix on GPU generation time: {torch_cuda_time:.4f} seconds\")\n",
    "    # print the size and non-zero ratio of the torch matrix\n",
    "    print(f\"Torch matrix size: {torch_matrix_cuda.shape}\")\n",
    "    print(f\"Torch matrix non-zero ratio: {torch_matrix_cuda._nnz()/torch_matrix_cuda.shape[0]/torch_matrix_cuda.shape[1]:.4f}\")\n",
    "    print(f\"Speedup factor: {np_time/torch_cuda_time:.2f}x\")\n",
    "    \n",
    "    \n",
    "    # benchmark the time it takes to matmul a dense random matrix\n",
    "    featmat = torch.randn(n_samples, n_features,)\n",
    "    # benchmark the time it takes to matmul with numpy\n",
    "    t0 = time.time()\n",
    "    featmat.numpy() @ np_matrix.T\n",
    "    np_time = time.time() - t0\n",
    "    print(f\"Numpy matrix matmul time: {np_time:.4f} seconds\")\n",
    "    \n",
    "    # benchmark the time it takes to matmul with torch matrix on CPU\n",
    "    t0 = time.time()\n",
    "    featmat @ torch_matrix.T\n",
    "    torch_cpu_time = time.time() - t0\n",
    "    print(f\"Torch matrix on CPU matmul time: {torch_cpu_time:.4f} seconds\")\n",
    "    t0 = time.time()\n",
    "    torch.sparse.mm(featmat, torch_matrix.T)\n",
    "    torch_cpu_time = time.time() - t0\n",
    "    print(f\"Torch matrix on CPU sparse matmul time: {torch_cpu_time:.4f} seconds\")\n",
    "    \n",
    "    # benchmark the time it takes to matmul with torch matrix on GPU\n",
    "    t0 = time.time()\n",
    "    featmat.to(\"cuda\") @ torch_matrix_cuda.T\n",
    "    torch_cuda_time = time.time() - t0\n",
    "    print(f\"Torch matrix on GPU matmul time: {torch_cuda_time:.4f} seconds\")\n",
    "    t0 = time.time()\n",
    "    torch.sparse.mm(featmat.to(\"cuda\"), torch_matrix_cuda.T)\n",
    "    torch_cuda_time = time.time() - t0\n",
    "    print(f\"Torch matrix on GPU sparse matmul time: {torch_cuda_time:.4f} seconds\")\n",
    "    print(f\"Speedup factor: {np_time/torch_cuda_time:.2f}x\")\n",
    "    return np_matrix, torch_matrix, torch_matrix_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy matrix generation time: 4.6529 seconds\n",
      "Numpy matrix size: (5920, 50000)\n",
      "Numpy matrix non-zero ratio: 0.0045\n",
      "Torch matrix on CPU generation time: 2.8734 seconds\n",
      "Torch matrix size: torch.Size([5920, 50000])\n",
      "Torch matrix non-zero ratio: 0.0045\n",
      "Torch matrix on GPU generation time: 1.8920 seconds\n",
      "Torch matrix size: torch.Size([5920, 50000])\n",
      "Torch matrix non-zero ratio: 0.0045\n",
      "Speedup factor: 2.46x\n",
      "Numpy matrix matmul time: 1.1698 seconds\n",
      "Torch matrix on CPU matmul time: 7.2082 seconds\n",
      "Torch matrix on GPU matmul time: 0.0211 seconds\n",
      "Speedup factor: 55.35x\n"
     ]
    }
   ],
   "source": [
    "benchmark_sparse_random_projection(n_samples = 1000,\n",
    "                                   n_components = \"auto\",\n",
    "                                   n_features = 50000,\n",
    "                                   density = \"auto\",\n",
    "                                   random_state = 42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy matrix generation time: 47.9755 seconds\n",
      "Numpy matrix size: (5920, 500000)\n",
      "Numpy matrix non-zero ratio: 0.0014\n",
      "Torch matrix on CPU generation time: 23.3475 seconds\n",
      "Torch matrix size: torch.Size([5920, 500000])\n",
      "Torch matrix non-zero ratio: 0.0014\n",
      "Torch matrix on GPU generation time: 1.9327 seconds\n",
      "Torch matrix size: torch.Size([5920, 500000])\n",
      "Torch matrix non-zero ratio: 0.0014\n",
      "Speedup factor: 24.82x\n",
      "Numpy matrix matmul time: 5.4424 seconds\n",
      "Torch matrix on CPU matmul time: 35.4209 seconds\n",
      "Torch matrix on GPU matmul time: 0.2012 seconds\n",
      "Speedup factor: 27.05x\n"
     ]
    }
   ],
   "source": [
    "benchmark_sparse_random_projection(n_samples = 1000,\n",
    "                                   n_components = \"auto\",\n",
    "                                   n_features = 500000,\n",
    "                                   density = \"auto\",\n",
    "                                   random_state = 42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking SparseRandomProjection with n_samples=1000, n_features=50000, n_components=auto, eps=0.1, random_state=42\n",
    "* Sklearn time: 3.5581 seconds\n",
    "* Torch time: 1.9403 seconds\n",
    "* Speedup factor: 1.83x\n",
    "\n",
    "#### Benchmarking SparseRandomProjection with n_samples=1000, n_features=500000, n_components=auto, eps=0.1, random_state=42\n",
    "* Sklearn time: 12.5077 seconds\n",
    "* Torch time: 2.5684 seconds\n",
    "* Speedup factor: 4.87x\n",
    "--------------------------------\n",
    "#### Benchmarking sparse matrix generation and matmul with n_samples=1000, n_features=50000, n_components=auto, density=auto, random_state=42\n",
    "* Numpy matrix generation time: 2.8102 seconds\n",
    "* Numpy matrix size: (5920, 50000)\n",
    "* Numpy matrix non-zero ratio: 0.0045\n",
    "* Torch matrix on CPU generation time: 2.7074 seconds\n",
    "* Torch matrix size: torch.Size([5920, 50000])\n",
    "* Torch matrix non-zero ratio: 0.0045\n",
    "* Torch matrix on GPU generation time: 1.7917 seconds\n",
    "* Torch matrix size: torch.Size([5920, 50000])\n",
    "* Torch matrix non-zero ratio: 0.0045\n",
    "* Speedup factor: 1.57x\n",
    "\n",
    "* Numpy matrix matmul time: 1.1104 seconds\n",
    "* Torch matrix on CPU matmul time: 7.0737 seconds\n",
    "* Torch matrix on CPU sparse matmul time: 7.0749 seconds\n",
    "* Torch matrix on GPU matmul time: 0.0197 seconds\n",
    "* Torch matrix on GPU sparse matmul time: 0.0364 seconds\n",
    "* Speedup factor: 30.48x\n",
    "\n",
    "#### Benchmarking sparse matrix generation and matmul with n_samples=1000, n_features=500000, n_components=auto, density=auto, random_state=42\n",
    "* Numpy matrix generation time: 8.8679 seconds\n",
    "* Numpy matrix size: (5920, 500000)\n",
    "* Numpy matrix non-zero ratio: 0.0014\n",
    "* Torch matrix on CPU generation time: 22.1904 seconds\n",
    "* Torch matrix size: torch.Size([5920, 500000])\n",
    "* Torch matrix non-zero ratio: 0.0014\n",
    "* Torch matrix on GPU generation time: 1.8465 seconds\n",
    "* Torch matrix size: torch.Size([5920, 500000])\n",
    "* Torch matrix non-zero ratio: 0.0014\n",
    "* Speedup factor: 4.80x\n",
    "\n",
    "* Numpy matrix matmul time: 5.2763 seconds\n",
    "* Torch matrix on CPU matmul time: 34.7338 seconds\n",
    "* Torch matrix on CPU sparse matmul time: 34.6512 seconds\n",
    "* Torch matrix on GPU matmul time: 0.1881 seconds\n",
    "* Torch matrix on GPU sparse matmul time: 0.2777 seconds\n",
    "* Speedup factor: 19.00x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "* Generally, generating sparse random matrix is faster through torch cpu than numpy, much faster on CUDA (2x to 24x speedup). \n",
    "* However, matrix multiplication in torch cpu is slower (x5,x7 slow down)! Potentially due to non optimized sparse matmul implementation. \n",
    "* But matmul with sparse weights on CUDA is much faster 27x to 55x speed up than the numpy sparse matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtsr = torch.randn(1000, 500000, device=\"cuda\")\n",
    "projection_matrix = _sparse_random_matrix_torch(2500, 500000, density=\"auto\", random_state=42, device=\"cuda\", use_multinomial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.9790,   3.4008,  -9.6979,  ..., -10.0966,  -4.7390,  20.8956],\n",
       "        [-19.4450, -11.0050,   6.1506,  ...,   6.2979,  -7.8975,   0.6770],\n",
       "        [-29.1357,  11.3054, -13.5277,  ...,  35.4753,   4.0125,  -7.0463],\n",
       "        ...,\n",
       "        [ -6.6439,  33.0906, -14.1929,  ...,  -4.3143,  -2.8030,  -2.1197],\n",
       "        [  1.1072,   5.0099,  -2.1067,  ...,   2.5069,  16.5518,   1.1054],\n",
       "        [ -9.8507,  28.1183, -12.2733,  ...,  -9.0177,   2.4313, -17.1498]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtsr @ projection_matrix.T\n",
    "\n",
    "torch.sparse.mm(Xtsr, projection_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.9790,   3.4008,  -9.6979,  ..., -10.0966,  -4.7390,  20.8956],\n",
       "        [-19.4450, -11.0050,   6.1506,  ...,   6.2979,  -7.8975,   0.6770],\n",
       "        [-29.1357,  11.3054, -13.5277,  ...,  35.4753,   4.0125,  -7.0463],\n",
       "        ...,\n",
       "        [ -6.6439,  33.0906, -14.1929,  ...,  -4.3143,  -2.8030,  -2.1197],\n",
       "        [  1.1072,   5.0099,  -2.1067,  ...,   2.5069,  16.5518,   1.1054],\n",
       "        [ -9.8507,  28.1183, -12.2733,  ...,  -9.0177,   2.4313, -17.1498]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference between torch and sklearn projections: 1.7166138e-05\n",
      "Test passed: torch projection matches sklearn projection!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "def torch_to_sklearn_projection(torch_sparse, n_components, ):\n",
    "    # Move to CPU and ensure the tensor is coalesced\n",
    "    torch_sparse = torch_sparse.cpu().coalesce()\n",
    "    # Extract indices and values from the torch sparse tensor.\n",
    "    indices = torch_sparse.indices().numpy()\n",
    "    values = torch_sparse.values().numpy()\n",
    "    shape = torch_sparse.size()  # Expected shape: (n_components, n_features)\n",
    "    # Create a scipy sparse matrix (using COO and converting to CSR for efficiency)\n",
    "    scipy_random_matrix = sp.coo_matrix((values, (indices[0], indices[1])), shape=shape).tocsr()\n",
    "    # Initialize a SparseRandomProjection instance.\n",
    "    srp = SparseRandomProjection(n_components=n_components, dense_output=True)\n",
    "    # Assign the random projection matrix to the sklearn object.\n",
    "    srp.components_ = scipy_random_matrix\n",
    "    return srp\n",
    "\n",
    "\n",
    "def test_projection_consistency(n_components = 1000, \n",
    "                                n_features = 500000, \n",
    "                                n_samples = 1000, \n",
    "                                density = 0.0014,   \n",
    "                                random_state = 42):\n",
    "    # Define dimensions.\n",
    "    # Adjust density for the sparse projection matrix.\n",
    "    # Set random seeds for reproducibility.\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    # Create a random sparse projection matrix in torch.\n",
    "    # Here, we simulate a matrix with values either +0.3761 or -0.3761.\n",
    "    total_elements = n_components * n_features\n",
    "    nnz = int(total_elements * density)\n",
    "    # Randomly pick indices for non-zero entries.\n",
    "    indices_0 = torch.randint(0, n_components, (nnz,))\n",
    "    indices_1 = torch.randint(0, n_features, (nnz,))\n",
    "    indices = torch.stack([indices_0, indices_1], dim=0)\n",
    "    # Randomly choose values +0.3761 or -0.3761.\n",
    "    values = torch.empty(nnz).uniform_()  # uniform random values in [0,1)\n",
    "    values = torch.where(values < 0.5, torch.tensor(-0.3761), torch.tensor(0.3761))\n",
    "    # Create the sparse tensor.\n",
    "    torch_sparse = torch.sparse_coo_tensor(indices, values, size=(n_components, n_features))\n",
    "    # Convert torch sparse matrix to sklearn random projection.\n",
    "    srp = torch_to_sklearn_projection(torch_sparse, n_components, n_features)\n",
    "    # Generate a random input data matrix X.\n",
    "    X_np = np.random.randn(n_samples, n_features).astype(np.float32)\n",
    "    X_torch = torch.from_numpy(X_np)\n",
    "    # Compute projection with sklearn: X * components_.T.\n",
    "    X_proj_sklearn = srp.transform(X_np)\n",
    "    # Compute projection with torch.\n",
    "    # Since torch_sparse has shape (n_components, n_features), we need its transpose for X * A.T.\n",
    "    torch_sparse_transposed = torch_sparse.transpose(0, 1).coalesce()  # shape: (n_features, n_components)\n",
    "    # Perform sparse-dense matrix multiplication.\n",
    "    X_proj_torch = torch.sparse.mm(X_torch, torch_sparse_transposed).numpy()  # shape: (n_samples, n_components)\n",
    "    # Compare results.\n",
    "    diff = np.abs(X_proj_torch - X_proj_sklearn).max()\n",
    "    print(\"Maximum difference between torch and sklearn projections:\", diff)\n",
    "    # Assert that the results are nearly equal.\n",
    "    assert np.allclose(X_proj_torch, X_proj_sklearn, atol=1e-4), \"Results are different!\"\n",
    "    print(\"Test passed: torch projection matches sklearn projection!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_projection_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[     0,      0,      0,  ...,   4999,   4999,   4999],\n",
       "                       [   415,    472,    767,  ..., 497710, 499147, 499828]]),\n",
       "       values=tensor([ 0.3761, -0.3761, -0.3761,  ...,  0.3761,  0.3761,\n",
       "                       0.3761]),\n",
       "       device='cuda:0', size=(5000, 500000), nnz=3531808,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_matrix_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected feature matrix shape with specified n_components: torch.Size([100, 10])\n",
      "Projected feature matrix shape with default n_components: torch.Size([100, 7])\n",
      "Exported sklearn components_ shape: (10, 50)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "try:\n",
    "    from sklearn.random_projection import SparseRandomProjection, _sparse_random_matrix, johnson_lindenstrauss_min_dim\n",
    "except ImportError:\n",
    "    raise ImportError(\"scikit-learn must be installed to export to sklearn.\")\n",
    "\n",
    "class SparseRandomProjection_torch:\n",
    "    def __init__(self, n_components=None, random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          n_components : int, optional\n",
    "              The target dimensionality. If None, will be set during fitting (default: int(sqrt(n_features))).\n",
    "          random_state : int, optional\n",
    "              Seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.projection_matrix = None\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(random_state)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the random projection matrix based on input tensor X.\n",
    "        \n",
    "        Parameters:\n",
    "          X : torch.Tensor of shape (n_samples, n_features)\n",
    "          \n",
    "        Returns:\n",
    "          self\n",
    "        \"\"\"\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            raise ValueError(\"Input X must be a torch.Tensor\")\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        # Set default n_components if not provided\n",
    "        if self.n_components is None:\n",
    "            self.n_components = int(math.sqrt(n_features))\n",
    "            if self.n_components < 1:\n",
    "                self.n_components = 1\n",
    "\n",
    "        # Determine density and scale as in the sparse random projection literature.\n",
    "        # Density: probability that any given entry is nonzero.\n",
    "        density = 1.0 / math.sqrt(n_features)\n",
    "        # Scaling factor to maintain norm in expectation.\n",
    "        scale = math.sqrt(1.0 / density)  # equals n_features^(1/4)\n",
    "\n",
    "        # Create the random matrix\n",
    "        # Generate a mask with probability \"density\" for nonzero entries.\n",
    "        mask = (torch.rand(n_features, self.n_components) < density).float()\n",
    "        # Generate random signs (+1 or -1) for nonzero entries.\n",
    "        signs = torch.randint(0, 2, (n_features, self.n_components)).float() * 2 - 1\n",
    "        # The projection matrix: non-zero entries are ±scale, zeros otherwise.\n",
    "        self.projection_matrix = mask * signs * scale\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Project the data X using the learned projection matrix.\n",
    "        \n",
    "        Parameters:\n",
    "          X : torch.Tensor of shape (n_samples, n_features)\n",
    "          \n",
    "        Returns:\n",
    "          X_projected : torch.Tensor of shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        if self.projection_matrix is None:\n",
    "            raise RuntimeError(\"The model has not been fitted yet. Call 'fit' or 'fit_transform' first.\")\n",
    "        return torch.matmul(X, self.projection_matrix)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        \n",
    "        Parameters:\n",
    "          X : torch.Tensor of shape (n_samples, n_features)\n",
    "          \n",
    "        Returns:\n",
    "          X_projected : torch.Tensor of shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def export_to_sklearn(self):\n",
    "        \"\"\"\n",
    "        Exports the current projection matrix to a scikit-learn SparseRandomProjection instance.\n",
    "        The sklearn object will have its 'components_' attribute set to the projection matrix.\n",
    "        Note: The sklearn SparseRandomProjection expects components_ of shape (n_components, n_features),\n",
    "        so the projection matrix is transposed.\n",
    "        \n",
    "        Returns:\n",
    "          skrp : sklearn.random_projection.SparseRandomProjection instance with the learned components.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create an instance with the same n_components.\n",
    "        skrp = SparseRandomProjection(n_components=self.n_components)\n",
    "        \n",
    "        # The sklearn transformer stores components_ as a sparse matrix of shape (n_components, n_features).\n",
    "        # Convert the torch tensor to a numpy array and then to a scipy sparse matrix.\n",
    "        import scipy.sparse\n",
    "        \n",
    "        # Ensure projection_matrix is on CPU and convert to numpy.\n",
    "        proj_np = self.projection_matrix.cpu().numpy()\n",
    "        # Transpose so that shape becomes (n_components, n_features).\n",
    "        skrp.components_ = scipy.sparse.csc_matrix(proj_np.T)\n",
    "        return skrp\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Create a random feature matrix of shape (n_samples, n_features)\n",
    "    n_samples, n_features = 100, 50\n",
    "    featmat = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Using a specified n_components\n",
    "    n_components = 10\n",
    "    srp_transformer = SparseRandomProjection_torch(n_components=n_components, random_state=42)\n",
    "    featmat_srp = srp_transformer.fit_transform(featmat)\n",
    "    print(\"Projected feature matrix shape with specified n_components:\", featmat_srp.shape)\n",
    "    \n",
    "    # Using default n_components (automatically determined)\n",
    "    srp_transformer_default = SparseRandomProjection_torch(random_state=42)\n",
    "    featmat_srp_default = srp_transformer_default.fit_transform(featmat)\n",
    "    print(\"Projected feature matrix shape with default n_components:\", featmat_srp_default.shape)\n",
    "    \n",
    "    # Exporting to sklearn version:\n",
    "    sk_srp = srp_transformer.export_to_sklearn()\n",
    "    print(\"Exported sklearn components_ shape:\", sk_srp.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference between torch and sklearn projections: 23.561712\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Results are different!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(X_proj_torch\u001b[38;5;241m.\u001b[39mnumpy(), X_proj_sklearn, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults are different!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: torch projection matches sklearn projection!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtest_projection_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mtest_projection_consistency\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum difference between torch and sklearn projections:\u001b[39m\u001b[38;5;124m\"\u001b[39m, diff)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Assert that the results are nearly equal.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(X_proj_torch\u001b[38;5;241m.\u001b[39mnumpy(), X_proj_sklearn, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults are different!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: torch projection matches sklearn projection!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Results are different!"
     ]
    }
   ],
   "source": [
    "def test_projection_consistency():\n",
    "    \"\"\"\n",
    "    Test function to compare the PyTorch-based SparseRandomProjection with scikit-learn's implementation.\n",
    "    \n",
    "    It fits the torch-based transformer, exports its projection matrix to a sklearn object,\n",
    "    transforms the data using both methods, and asserts that the outputs are equal (within tolerance).\n",
    "    \"\"\"\n",
    "    # Fix random seed for reproducibility.\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate a random feature matrix.\n",
    "    n_samples, n_features = 100, 50\n",
    "    X_torch = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Instantiate and fit the torch-based transformer.\n",
    "    n_components = 10\n",
    "    srp_torch = SparseRandomProjection_torch(n_components=n_components, random_state=42)\n",
    "    X_proj_torch = srp_torch.fit_transform(X_torch)\n",
    "    \n",
    "    srp_sklearn = SparseRandomProjection(n_components=n_components, random_state=42)\n",
    "    X_proj_sklearn = srp_sklearn.fit_transform(X_torch.numpy())\n",
    "    \n",
    "    # Export to scikit-learn transformer.\n",
    "    # srp_sklearn = srp_torch.export_to_sklearn()\n",
    "    \n",
    "    # Transform the same data using the sklearn transformer.\n",
    "    # X_np = X_torch.numpy()\n",
    "    # X_proj_sklearn = srp_sklearn.transform(X_np)\n",
    "    \n",
    "    # Compute the maximum absolute difference between the two results.\n",
    "    diff = np.abs(X_proj_torch.numpy() - X_proj_sklearn).max()\n",
    "    print(\"Maximum difference between torch and sklearn projections:\", diff)\n",
    "    \n",
    "    # Assert that the results are nearly equal.\n",
    "    assert np.allclose(X_proj_torch.numpy(), X_proj_sklearn, atol=1e-6), \"Results are different!\"\n",
    "    print(\"Test passed: torch projection matches sklearn projection!\")\n",
    "\n",
    "test_projection_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
