{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/Closed-loop-visual-insilico\")\n",
    "import timm\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm.auto import tqdm\n",
    "from os.path import join\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from horama import maco, plot_maco\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Normalize, Resize\n",
    "from torchvision.models import resnet50\n",
    "from circuit_toolkit.CNN_scorers import TorchScorer\n",
    "from circuit_toolkit.GAN_utils import upconvGAN, Caffenet\n",
    "from circuit_toolkit.plot_utils import to_imgrid, show_imgrid, save_imgrid, saveallforms\n",
    "from circuit_toolkit.layer_hook_utils import featureFetcher_module, featureFetcher, get_module_names\n",
    "from circuit_toolkit.dataset_utils import ImagePathDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from neural_regress.regress_lib import sweep_regressors\n",
    "from neural_regress.sklearn_torchify_lib import SRP_torch, PCA_torch, LinearRegression_torch, SpatialAvg_torch\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.random_projection import SparseRandomProjection, GaussianRandomProjection\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "#%% Utility Functions\n",
    "from core.data_utils import load_from_hdf5, load_neural_data, load_neural_trial_resp_tensor, create_response_tensor, parse_image_fullpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_utils import load_neural_data, load_from_hdf5, load_neural_trial_resp_tensor, create_response_tensor\n",
    "dataroot = r\"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Ephys_Data\"\n",
    "data_path = join(dataroot, \"red_20250428-20240429_vvs-encodingstimuli_z1_rw100-400.h5\")\n",
    "data = load_from_hdf5(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All stimulus files were found\n"
     ]
    }
   ],
   "source": [
    "encoding_stim_dir = r\"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Stimuli/encodingstimuli_apr2025\"\n",
    "data_path = join(dataroot, \"red_20250428-20240429_vvs-encodingstimuli_z1_rw100-400.h5\")\n",
    "data = load_from_hdf5(data_path)\n",
    "\n",
    "subject_id = \"red_20250428-20240429\"\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['image_fps'] = parse_image_fullpaths(data['repavg'][\"stimulus_name\"], [encoding_stim_dir], arbitrary_format=True)\n",
    "data_dict['resp_mat'] = data['repavg'][\"response_peak\"]\n",
    "data_dict['resp_temp_mat'] = data['repavg'][\"response_temporal\"]\n",
    "data_dict['reliability'] = data['neuron_metadata'][\"reliability\"]\n",
    "data_dict['ncsnr'] = data['neuron_metadata'][\"ncsnr\"]\n",
    "data_dict['brain_area'] = data['neuron_metadata'][\"brain_area\"]\n",
    "data_dict['stim_pos'] = data['stimulus_meta']['xy_deg']\n",
    "data_dict['stim_size'] = data[\"stimulus_meta\"][\"size_px\"]\n",
    "\n",
    "image_fps = data_dict['image_fps']\n",
    "resp_mat = data_dict['resp_mat']\n",
    "reliability = data_dict['reliability']\n",
    "ncsnr = data_dict['ncsnr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/Closed-loop-visual-insilico\")\n",
    "from core.model_load_utils import load_model_transform, MODEL_LAYER_FILTERS, LAYER_ABBREVIATION_MAPS\n",
    "from neural_regress.regress_lib import record_features, perform_regression_sweeplayer, perform_regression_sweeplayer_RidgeCV\n",
    "from neural_regress.regress_lib import sweep_regressors, transform_features2Xdict, RidgeCV\n",
    "from neural_regress.regress_eval_lib import format_result_df, plot_result_df_per_layer, construct_result_df_masked, \\\n",
    "    compute_pred_dict_D2_per_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Testing model: resnet50_robust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: resnet50_robust\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for resnet50_robust with 969 images\n",
      "Successfully created feature fetcher for resnet50_robust\n",
      "Total number of modules: 184\n",
      "Number of layers passing filter: 16\n",
      "Filtered layer names: ['.layer1.Bottleneck0', '.layer1.Bottleneck1', '.layer1.Bottleneck2', '.layer2.Bottleneck0', '.layer2.Bottleneck1', '.layer2.Bottleneck2', '.layer2.Bottleneck3', '.layer3.Bottleneck0', '.layer3.Bottleneck1', '.layer3.Bottleneck2', '.layer3.Bottleneck3', '.layer3.Bottleneck4', '.layer3.Bottleneck5', '.layer4.Bottleneck0', '.layer4.Bottleneck1', '.layer4.Bottleneck2']\n",
      "Filtered layer names (abbreviated): ['L1.B0', 'L1.B1', 'L1.B2', 'L2.B0', 'L2.B1', 'L2.B2', 'L2.B3', 'L3.B0', 'L3.B1', 'L3.B2', 'L3.B3', 'L3.B4', 'L3.B5', 'L4.B0', 'L4.B1', 'L4.B2']\n",
      "\n",
      "Testing model: resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: resnet50\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for resnet50 with 969 images\n",
      "Successfully created feature fetcher for resnet50\n",
      "Total number of modules: 184\n",
      "Number of layers passing filter: 16\n",
      "Filtered layer names: ['.layer1.Bottleneck0', '.layer1.Bottleneck1', '.layer1.Bottleneck2', '.layer2.Bottleneck0', '.layer2.Bottleneck1', '.layer2.Bottleneck2', '.layer2.Bottleneck3', '.layer3.Bottleneck0', '.layer3.Bottleneck1', '.layer3.Bottleneck2', '.layer3.Bottleneck3', '.layer3.Bottleneck4', '.layer3.Bottleneck5', '.layer4.Bottleneck0', '.layer4.Bottleneck1', '.layer4.Bottleneck2']\n",
      "Filtered layer names (abbreviated): ['L1.B0', 'L1.B1', 'L1.B2', 'L2.B0', 'L2.B1', 'L2.B2', 'L2.B3', 'L3.B0', 'L3.B1', 'L3.B2', 'L3.B3', 'L3.B4', 'L3.B5', 'L4.B0', 'L4.B1', 'L4.B2']\n",
      "\n",
      "Testing model: resnet50_clip\n",
      "Successfully loaded model: resnet50_clip\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for resnet50_clip with 969 images\n",
      "Successfully created feature fetcher for resnet50_clip\n",
      "Total number of modules: 209\n",
      "Number of layers passing filter: 16\n",
      "Filtered layer names: ['.layer1.Bottleneck0', '.layer1.Bottleneck1', '.layer1.Bottleneck2', '.layer2.Bottleneck0', '.layer2.Bottleneck1', '.layer2.Bottleneck2', '.layer2.Bottleneck3', '.layer3.Bottleneck0', '.layer3.Bottleneck1', '.layer3.Bottleneck2', '.layer3.Bottleneck3', '.layer3.Bottleneck4', '.layer3.Bottleneck5', '.layer4.Bottleneck0', '.layer4.Bottleneck1', '.layer4.Bottleneck2']\n",
      "Filtered layer names (abbreviated): ['L1.B0', 'L1.B1', 'L1.B2', 'L2.B0', 'L2.B1', 'L2.B2', 'L2.B3', 'L3.B0', 'L3.B1', 'L3.B2', 'L3.B3', 'L3.B4', 'L3.B5', 'L4.B0', 'L4.B1', 'L4.B2']\n",
      "\n",
      "Testing model: resnet50_dino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: resnet50_dino\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for resnet50_dino with 969 images\n",
      "Successfully created feature fetcher for resnet50_dino\n",
      "Total number of modules: 184\n",
      "Number of layers passing filter: 16\n",
      "Filtered layer names: ['.layer1.Bottleneck0', '.layer1.Bottleneck1', '.layer1.Bottleneck2', '.layer2.Bottleneck0', '.layer2.Bottleneck1', '.layer2.Bottleneck2', '.layer2.Bottleneck3', '.layer3.Bottleneck0', '.layer3.Bottleneck1', '.layer3.Bottleneck2', '.layer3.Bottleneck3', '.layer3.Bottleneck4', '.layer3.Bottleneck5', '.layer4.Bottleneck0', '.layer4.Bottleneck1', '.layer4.Bottleneck2']\n",
      "Filtered layer names (abbreviated): ['L1.B0', 'L1.B1', 'L1.B2', 'L2.B0', 'L2.B1', 'L2.B2', 'L2.B3', 'L3.B0', 'L3.B1', 'L3.B2', 'L3.B3', 'L3.B4', 'L3.B5', 'L4.B0', 'L4.B1', 'L4.B2']\n",
      "\n",
      "Testing model: clipag_vitb32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: clipag_vitb32\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for clipag_vitb32 with 969 images\n",
      "Successfully created feature fetcher for clipag_vitb32\n",
      "Total number of modules: 127\n",
      "Number of layers passing filter: 12\n",
      "Filtered layer names: ['.transformer.resblocks.ResidualAttentionBlock0', '.transformer.resblocks.ResidualAttentionBlock1', '.transformer.resblocks.ResidualAttentionBlock2', '.transformer.resblocks.ResidualAttentionBlock3', '.transformer.resblocks.ResidualAttentionBlock4', '.transformer.resblocks.ResidualAttentionBlock5', '.transformer.resblocks.ResidualAttentionBlock6', '.transformer.resblocks.ResidualAttentionBlock7', '.transformer.resblocks.ResidualAttentionBlock8', '.transformer.resblocks.ResidualAttentionBlock9', '.transformer.resblocks.ResidualAttentionBlock10', '.transformer.resblocks.ResidualAttentionBlock11']\n",
      "Filtered layer names (abbreviated): ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11']\n",
      "\n",
      "Testing model: siglip2_vitb16\n",
      "Successfully loaded model: siglip2_vitb16\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for siglip2_vitb16 with 969 images\n",
      "Successfully created feature fetcher for siglip2_vitb16\n",
      "Total number of modules: 270\n",
      "Number of layers passing filter: 13\n",
      "Filtered layer names: ['.trunk.blocks.Block0', '.trunk.blocks.Block1', '.trunk.blocks.Block2', '.trunk.blocks.Block3', '.trunk.blocks.Block4', '.trunk.blocks.Block5', '.trunk.blocks.Block6', '.trunk.blocks.Block7', '.trunk.blocks.Block8', '.trunk.blocks.Block9', '.trunk.blocks.Block10', '.trunk.blocks.Block11', '.trunk.AttentionPoolLatentattn_pool']\n",
      "Filtered layer names (abbreviated): ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11', 'attnpool']\n",
      "\n",
      "Testing model: dinov2_vitb14_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: dinov2_vitb14_reg\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for dinov2_vitb14_reg with 969 images\n",
      "Successfully created feature fetcher for dinov2_vitb14_reg\n",
      "Total number of modules: 187\n",
      "Number of layers passing filter: 12\n",
      "Filtered layer names: ['.blocks.NestedTensorBlock0', '.blocks.NestedTensorBlock1', '.blocks.NestedTensorBlock2', '.blocks.NestedTensorBlock3', '.blocks.NestedTensorBlock4', '.blocks.NestedTensorBlock5', '.blocks.NestedTensorBlock6', '.blocks.NestedTensorBlock7', '.blocks.NestedTensorBlock8', '.blocks.NestedTensorBlock9', '.blocks.NestedTensorBlock10', '.blocks.NestedTensorBlock11']\n",
      "Filtered layer names (abbreviated): ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11']\n",
      "\n",
      "Testing model: radio_v2.5-b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/NVlabs_RADIO_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: radio_v2.5-b\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for radio_v2.5-b with 969 images\n",
      "Successfully created feature fetcher for radio_v2.5-b\n",
      "Total number of modules: 251\n",
      "Number of layers passing filter: 12\n",
      "Filtered layer names: ['.model.blocks.Block0', '.model.blocks.Block1', '.model.blocks.Block2', '.model.blocks.Block3', '.model.blocks.Block4', '.model.blocks.Block5', '.model.blocks.Block6', '.model.blocks.Block7', '.model.blocks.Block8', '.model.blocks.Block9', '.model.blocks.Block10', '.model.blocks.Block11']\n",
      "Filtered layer names (abbreviated): ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11']\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "batch_size = 96\n",
    "\n",
    "model_names = [\n",
    "    \"resnet50_robust\",\n",
    "    \"resnet50\",\n",
    "    \"resnet50_clip\",\n",
    "    \"resnet50_dino\",\n",
    "    \"clipag_vitb32\",\n",
    "    \"siglip2_vitb16\",\n",
    "    \"dinov2_vitb14_reg\",\n",
    "    \"radio_v2.5-b\",\n",
    "]\n",
    "\n",
    "for modelname in model_names:\n",
    "    print(f\"\\nTesting model: {modelname}\")\n",
    "    try:\n",
    "        # Load model and transforms\n",
    "        model, transforms_pipeline = load_model_transform(modelname, device)\n",
    "        print(f\"Successfully loaded model: {modelname}\")\n",
    "        dataset = ImagePathDataset(image_fps, scores=None, transform=transforms_pipeline)\n",
    "        print(f\"Successfully created dataset for {modelname} with {len(dataset)} images\")\n",
    "        # Create feature fetcher\n",
    "        fetcher = featureFetcher(model, input_size=(3, 224, 224), print_module=False)\n",
    "        print(f\"Successfully created feature fetcher for {modelname}\")\n",
    "        # Get all module names\n",
    "        all_module_names = list(fetcher.module_names.values())\n",
    "        print(f\"Total number of modules: {len(all_module_names)}\")\n",
    "        # Get layer filter for this model\n",
    "        layer_filter = MODEL_LAYER_FILTERS[modelname]\n",
    "        layer_abbrev = LAYER_ABBREVIATION_MAPS[modelname]\n",
    "        # Count layers that pass the filter\n",
    "        module_names2record = [name for name in all_module_names if layer_filter(name)]\n",
    "        print(f\"Number of layers passing filter: {len(module_names2record)}\")\n",
    "        print(f\"Filtered layer names: {module_names2record}\")\n",
    "        module_names2record_abbrev = [layer_abbrev(name) for name in module_names2record]\n",
    "        print(f\"Filtered layer names (abbreviated): {module_names2record_abbrev}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {modelname}: {str(e)}\")\n",
    "    \n",
    "    # # hook the layers\n",
    "    # for name in module_names2record: \n",
    "    #     fetcher.record(name, store_device='cpu', ingraph=False, )\n",
    "    \n",
    "    # # Record features\n",
    "    # feat_dict_lyrswp = record_features(model, fetcher, dataset, batch_size=batch_size, device=device)\n",
    "    # # Cleanup\n",
    "    # print(f\"{modelname} done!!!\")\n",
    "    # fetcher.cleanup()x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding_model_outputs\tEphys_Data  Reduced_Data  Stimuli\n"
     ]
    }
   ],
   "source": [
    "!ls /n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Encoding_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stim = pd.read_csv(join(encoding_stim_dir, \"encoding_stimuli_split_seed0.csv\"), )\n",
    "train_idx = df_stim[df_stim[\"is_train\"]].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  0,   1,   2,   3,   4,   5,   7,   8,  10,  11,\n",
       "       ...\n",
       "       957, 958, 959, 960, 961, 962, 964, 965, 966, 968],\n",
       "      dtype='int64', length=774)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Testing model: resnet50_robust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: resnet50_robust\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n",
      "Successfully created dataset for resnet50_robust with 969 images\n",
      "Successfully created feature fetcher for resnet50_robust\n",
      "Total number of modules: 184\n",
      "Number of layers passing filter: 16\n",
      "Filtered layer names: ['.layer1.Bottleneck0', '.layer1.Bottleneck1', '.layer1.Bottleneck2', '.layer2.Bottleneck0', '.layer2.Bottleneck1', '.layer2.Bottleneck2', '.layer2.Bottleneck3', '.layer3.Bottleneck0', '.layer3.Bottleneck1', '.layer3.Bottleneck2', '.layer3.Bottleneck3', '.layer3.Bottleneck4', '.layer3.Bottleneck5', '.layer4.Bottleneck0', '.layer4.Bottleneck1', '.layer4.Bottleneck2']\n",
      "Filtered layer names (abbreviated): ['L1.B0', 'L1.B1', 'L1.B2', 'L2.B0', 'L2.B1', 'L2.B2', 'L2.B3', 'L3.B0', 'L3.B1', 'L3.B2', 'L3.B3', 'L3.B4', 'L3.B5', 'L4.B0', 'L4.B1', 'L4.B2']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73849be991af43d2848f19fbb2982287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".layer1.Bottleneck0 torch.Size([969, 256, 56, 56])\n",
      ".layer1.Bottleneck1 torch.Size([969, 256, 56, 56])\n",
      ".layer1.Bottleneck2 torch.Size([969, 256, 56, 56])\n",
      ".layer2.Bottleneck0 torch.Size([969, 512, 28, 28])\n",
      ".layer2.Bottleneck1 torch.Size([969, 512, 28, 28])\n",
      ".layer2.Bottleneck2 torch.Size([969, 512, 28, 28])\n",
      ".layer2.Bottleneck3 torch.Size([969, 512, 28, 28])\n",
      ".layer3.Bottleneck0 torch.Size([969, 1024, 14, 14])\n",
      ".layer3.Bottleneck1 torch.Size([969, 1024, 14, 14])\n",
      ".layer3.Bottleneck2 torch.Size([969, 1024, 14, 14])\n",
      ".layer3.Bottleneck3 torch.Size([969, 1024, 14, 14])\n",
      ".layer3.Bottleneck4 torch.Size([969, 1024, 14, 14])\n",
      ".layer3.Bottleneck5 torch.Size([969, 1024, 14, 14])\n",
      ".layer4.Bottleneck0 torch.Size([969, 2048, 7, 7])\n",
      ".layer4.Bottleneck1 torch.Size([969, 2048, 7, 7])\n",
      ".layer4.Bottleneck2 torch.Size([969, 2048, 7, 7])\n",
      "resnet50_robust done!!!\n",
      "FeatureFetcher hooks all freed\n",
      "Fitting models for All channels N=64\n",
      ".layer1.Bottleneck0 torch.Size([969, 256, 56, 56])\n",
      "Time taken to transform .layer1.Bottleneck0 pca750 [969, 750]: 21.830s\n",
      "Time taken to transform .layer1.Bottleneck0 srp [969, 5893]: 5.989s\n",
      "Time taken to transform .layer1.Bottleneck0: 28.584s\n",
      ".layer1.Bottleneck1 torch.Size([969, 256, 56, 56])\n",
      "Time taken to transform .layer1.Bottleneck1 pca750 [969, 750]: 19.608s\n",
      "Time taken to transform .layer1.Bottleneck1 srp [969, 5893]: 3.212s\n",
      "Time taken to transform .layer1.Bottleneck1: 23.518s\n",
      ".layer1.Bottleneck2 torch.Size([969, 256, 56, 56])\n",
      "Time taken to transform .layer1.Bottleneck2 pca750 [969, 750]: 19.696s\n",
      "Time taken to transform .layer1.Bottleneck2 srp [969, 5893]: 3.378s\n",
      "Time taken to transform .layer1.Bottleneck2: 23.791s\n",
      ".layer2.Bottleneck0 torch.Size([969, 512, 28, 28])\n",
      "Time taken to transform .layer2.Bottleneck0 pca750 [969, 750]: 9.959s\n",
      "Time taken to transform .layer2.Bottleneck0 srp [969, 5893]: 2.547s\n",
      "Time taken to transform .layer2.Bottleneck0: 12.867s\n",
      ".layer2.Bottleneck1 torch.Size([969, 512, 28, 28])\n",
      "Time taken to transform .layer2.Bottleneck1 pca750 [969, 750]: 9.794s\n",
      "Time taken to transform .layer2.Bottleneck1 srp [969, 5893]: 2.539s\n",
      "Time taken to transform .layer2.Bottleneck1: 12.688s\n",
      ".layer2.Bottleneck2 torch.Size([969, 512, 28, 28])\n",
      "Time taken to transform .layer2.Bottleneck2 pca750 [969, 750]: 9.802s\n",
      "Time taken to transform .layer2.Bottleneck2 srp [969, 5893]: 2.564s\n",
      "Time taken to transform .layer2.Bottleneck2: 12.718s\n",
      ".layer2.Bottleneck3 torch.Size([969, 512, 28, 28])\n",
      "Time taken to transform .layer2.Bottleneck3 pca750 [969, 750]: 9.827s\n",
      "Time taken to transform .layer2.Bottleneck3 srp [969, 5893]: 2.529s\n",
      "Time taken to transform .layer2.Bottleneck3: 12.712s\n",
      ".layer3.Bottleneck0 torch.Size([969, 1024, 14, 14])\n",
      "Time taken to transform .layer3.Bottleneck0 pca750 [969, 750]: 4.867s\n",
      "Time taken to transform .layer3.Bottleneck0 srp [969, 5893]: 2.296s\n",
      "Time taken to transform .layer3.Bottleneck0: 7.341s\n",
      ".layer3.Bottleneck1 torch.Size([969, 1024, 14, 14])\n",
      "Time taken to transform .layer3.Bottleneck1 pca750 [969, 750]: 4.871s\n",
      "Time taken to transform .layer3.Bottleneck1 srp [969, 5893]: 2.306s\n",
      "Time taken to transform .layer3.Bottleneck1: 7.353s\n",
      ".layer3.Bottleneck2 torch.Size([969, 1024, 14, 14])\n",
      "Time taken to transform .layer3.Bottleneck2 pca750 [969, 750]: 4.870s\n",
      "Time taken to transform .layer3.Bottleneck2 srp [969, 5893]: 2.305s\n",
      "Time taken to transform .layer3.Bottleneck2: 7.350s\n",
      ".layer3.Bottleneck3 torch.Size([969, 1024, 14, 14])\n",
      "Time taken to transform .layer3.Bottleneck3 pca750 [969, 750]: 4.866s\n",
      "Time taken to transform .layer3.Bottleneck3 srp [969, 5893]: 2.291s\n",
      "Time taken to transform .layer3.Bottleneck3: 7.332s\n",
      ".layer3.Bottleneck4 torch.Size([969, 1024, 14, 14])\n",
      "Time taken to transform .layer3.Bottleneck4 pca750 [969, 750]: 4.868s\n",
      "Time taken to transform .layer3.Bottleneck4 srp [969, 5893]: 2.310s\n",
      "Time taken to transform .layer3.Bottleneck4: 7.354s\n",
      ".layer3.Bottleneck5 torch.Size([969, 1024, 14, 14])\n",
      "Time taken to transform .layer3.Bottleneck5 pca750 [969, 750]: 4.869s\n",
      "Time taken to transform .layer3.Bottleneck5 srp [969, 5893]: 2.304s\n",
      "Time taken to transform .layer3.Bottleneck5: 7.349s\n",
      ".layer4.Bottleneck0 torch.Size([969, 2048, 7, 7])\n",
      "Time taken to transform .layer4.Bottleneck0 pca750 [969, 750]: 2.431s\n",
      "Time taken to transform .layer4.Bottleneck0 srp [969, 5893]: 2.144s\n",
      "Time taken to transform .layer4.Bottleneck0: 4.664s\n",
      ".layer4.Bottleneck1 torch.Size([969, 2048, 7, 7])\n",
      "Time taken to transform .layer4.Bottleneck1 pca750 [969, 750]: 2.432s\n",
      "Time taken to transform .layer4.Bottleneck1 srp [969, 5893]: 2.143s\n",
      "Time taken to transform .layer4.Bottleneck1: 4.665s\n",
      ".layer4.Bottleneck2 torch.Size([969, 2048, 7, 7])\n",
      "Time taken to transform .layer4.Bottleneck2 pca750 [969, 750]: 2.432s\n",
      "Time taken to transform .layer4.Bottleneck2 srp [969, 5893]: 2.166s\n",
      "Time taken to transform .layer4.Bottleneck2: 4.686s\n",
      "Time taken to transform all features: 184.973s\n",
      ".layer1.Bottleneck0_pca750 RidgeCV D2_train: 0.220 D2_test: 0.052 time: 0.535\n",
      ".layer1.Bottleneck0_srp RidgeCV D2_train: 0.211 D2_test: 0.048 time: 0.299\n",
      ".layer1.Bottleneck1_pca750 RidgeCV D2_train: 0.268 D2_test: 0.056 time: 0.246\n",
      ".layer1.Bottleneck1_srp RidgeCV D2_train: 0.263 D2_test: 0.055 time: 0.274\n",
      ".layer1.Bottleneck2_pca750 RidgeCV D2_train: 0.252 D2_test: 0.054 time: 0.245\n",
      ".layer1.Bottleneck2_srp RidgeCV D2_train: 0.253 D2_test: 0.057 time: 0.272\n",
      ".layer2.Bottleneck0_pca750 RidgeCV D2_train: 0.242 D2_test: 0.060 time: 0.248\n",
      ".layer2.Bottleneck0_srp RidgeCV D2_train: 0.249 D2_test: 0.064 time: 0.274\n",
      ".layer2.Bottleneck1_pca750 RidgeCV D2_train: 0.249 D2_test: 0.072 time: 0.244\n",
      ".layer2.Bottleneck1_srp RidgeCV D2_train: 0.273 D2_test: 0.070 time: 0.277\n",
      ".layer2.Bottleneck2_pca750 RidgeCV D2_train: 0.271 D2_test: 0.072 time: 0.246\n",
      ".layer2.Bottleneck2_srp RidgeCV D2_train: 0.287 D2_test: 0.068 time: 0.278\n",
      ".layer2.Bottleneck3_pca750 RidgeCV D2_train: 0.287 D2_test: 0.074 time: 0.249\n",
      ".layer2.Bottleneck3_srp RidgeCV D2_train: 0.290 D2_test: 0.066 time: 0.282\n",
      ".layer3.Bottleneck0_pca750 RidgeCV D2_train: 0.279 D2_test: 0.091 time: 0.244\n",
      ".layer3.Bottleneck0_srp RidgeCV D2_train: 0.281 D2_test: 0.083 time: 0.282\n",
      ".layer3.Bottleneck1_pca750 RidgeCV D2_train: 0.269 D2_test: 0.092 time: 0.245\n",
      ".layer3.Bottleneck1_srp RidgeCV D2_train: 0.291 D2_test: 0.091 time: 0.276\n",
      ".layer3.Bottleneck2_pca750 RidgeCV D2_train: 0.272 D2_test: 0.095 time: 0.244\n",
      ".layer3.Bottleneck2_srp RidgeCV D2_train: 0.302 D2_test: 0.093 time: 0.278\n",
      ".layer3.Bottleneck3_pca750 RidgeCV D2_train: 0.283 D2_test: 0.097 time: 0.247\n",
      ".layer3.Bottleneck3_srp RidgeCV D2_train: 0.307 D2_test: 0.091 time: 0.281\n",
      ".layer3.Bottleneck4_pca750 RidgeCV D2_train: 0.278 D2_test: 0.102 time: 0.247\n",
      ".layer3.Bottleneck4_srp RidgeCV D2_train: 0.311 D2_test: 0.103 time: 0.283\n",
      ".layer3.Bottleneck5_pca750 RidgeCV D2_train: 0.308 D2_test: 0.108 time: 0.245\n",
      ".layer3.Bottleneck5_srp RidgeCV D2_train: 0.328 D2_test: 0.104 time: 0.280\n",
      ".layer4.Bottleneck0_pca750 RidgeCV D2_train: 0.327 D2_test: 0.129 time: 0.245\n",
      ".layer4.Bottleneck0_srp RidgeCV D2_train: 0.332 D2_test: 0.126 time: 0.279\n",
      ".layer4.Bottleneck1_pca750 RidgeCV D2_train: 0.312 D2_test: 0.131 time: 0.244\n",
      ".layer4.Bottleneck1_srp RidgeCV D2_train: 0.330 D2_test: 0.132 time: 0.278\n",
      ".layer4.Bottleneck2_pca750 RidgeCV D2_train: 0.303 D2_test: 0.142 time: 0.240\n",
      ".layer4.Bottleneck2_srp RidgeCV D2_train: 0.309 D2_test: 0.140 time: 0.278\n",
      "                                                                                alpha  \\\n",
      ".layer1.Bottleneck0_pca750 RidgeCV  [10000.0, 1000000000.0, 10000.0, 100000.0, 100...   \n",
      ".layer1.Bottleneck0_srp    RidgeCV  [10000.0, 1000000000.0, 100000.0, 1000000.0, 1...   \n",
      ".layer1.Bottleneck1_pca750 RidgeCV  [10000.0, 1000000000.0, 10000.0, 100000.0, 100...   \n",
      ".layer1.Bottleneck1_srp    RidgeCV  [10000.0, 1000000000.0, 100000.0, 100000.0, 10...   \n",
      ".layer1.Bottleneck2_pca750 RidgeCV  [10000.0, 1000000000.0, 100000.0, 1000000000.0...   \n",
      ".layer1.Bottleneck2_srp    RidgeCV  [10000.0, 1000000000.0, 100000.0, 1000000000.0...   \n",
      ".layer2.Bottleneck0_pca750 RidgeCV  [1000.0, 1000000000.0, 10000.0, 1000000000.0, ...   \n",
      ".layer2.Bottleneck0_srp    RidgeCV  [1000.0, 1000000000.0, 10000.0, 1000000000.0, ...   \n",
      ".layer2.Bottleneck1_pca750 RidgeCV  [10000.0, 1000000000.0, 10000.0, 1000000000.0,...   \n",
      ".layer2.Bottleneck1_srp    RidgeCV  [1000.0, 1000000000.0, 10000.0, 1000000000.0, ...   \n",
      ".layer2.Bottleneck2_pca750 RidgeCV  [10000.0, 1000000000.0, 10000.0, 1000000000.0,...   \n",
      ".layer2.Bottleneck2_srp    RidgeCV  [10000.0, 1000000000.0, 10000.0, 100000.0, 100...   \n",
      ".layer2.Bottleneck3_pca750 RidgeCV  [10000.0, 1000000000.0, 10000.0, 1000000000.0,...   \n",
      ".layer2.Bottleneck3_srp    RidgeCV  [10000.0, 1000000000.0, 100000.0, 1000000000.0...   \n",
      ".layer3.Bottleneck0_pca750 RidgeCV  [1000.0, 1000000000.0, 1000.0, 1000000000.0, 1...   \n",
      ".layer3.Bottleneck0_srp    RidgeCV  [1000.0, 1000000000.0, 10000.0, 100000.0, 1000...   \n",
      ".layer3.Bottleneck1_pca750 RidgeCV  [1000.0, 1000000000.0, 1000.0, 1000000000.0, 1...   \n",
      ".layer3.Bottleneck1_srp    RidgeCV  [1000.0, 1000000000.0, 10000.0, 1000000000.0, ...   \n",
      ".layer3.Bottleneck2_pca750 RidgeCV  [1000.0, 1000000000.0, 10000.0, 1000000000.0, ...   \n",
      ".layer3.Bottleneck2_srp    RidgeCV  [1000.0, 100000.0, 10000.0, 1000000000.0, 1000...   \n",
      ".layer3.Bottleneck3_pca750 RidgeCV  [1000.0, 1000000000.0, 10000.0, 1000000000.0, ...   \n",
      ".layer3.Bottleneck3_srp    RidgeCV  [1000.0, 1000000.0, 10000.0, 1000000000.0, 100...   \n",
      ".layer3.Bottleneck4_pca750 RidgeCV  [1000.0, 1000000000.0, 10000.0, 1000000000.0, ...   \n",
      ".layer3.Bottleneck4_srp    RidgeCV  [1000.0, 10000000.0, 1000.0, 1000000000.0, 100...   \n",
      ".layer3.Bottleneck5_pca750 RidgeCV  [1000.0, 1000000000.0, 1000.0, 1000000000.0, 1...   \n",
      ".layer3.Bottleneck5_srp    RidgeCV  [1000.0, 1000000000.0, 1000.0, 1000000000.0, 1...   \n",
      ".layer4.Bottleneck0_pca750 RidgeCV  [10000.0, 10000000.0, 10000.0, 1000000000.0, 1...   \n",
      ".layer4.Bottleneck0_srp    RidgeCV  [10000.0, 1000000000.0, 10000.0, 1000000000.0,...   \n",
      ".layer4.Bottleneck1_pca750 RidgeCV  [10000.0, 1000000.0, 10000.0, 1000000000.0, 10...   \n",
      ".layer4.Bottleneck1_srp    RidgeCV  [10000.0, 1000000.0, 10000.0, 1000000000.0, 10...   \n",
      ".layer4.Bottleneck2_pca750 RidgeCV  [10000.0, 1000000.0, 100000.0, 10000000.0, 100...   \n",
      ".layer4.Bottleneck2_srp    RidgeCV  [10000.0, 1000000.0, 100000.0, 10000000.0, 100...   \n",
      "\n",
      "                                   train_score test_score n_feat   runtime  \n",
      ".layer1.Bottleneck0_pca750 RidgeCV     0.22015   0.051813    750  0.535161  \n",
      ".layer1.Bottleneck0_srp    RidgeCV    0.210736    0.04837   5893  0.298729  \n",
      ".layer1.Bottleneck1_pca750 RidgeCV     0.26778   0.056247    750   0.24558  \n",
      ".layer1.Bottleneck1_srp    RidgeCV    0.262859   0.054725   5893  0.273521  \n",
      ".layer1.Bottleneck2_pca750 RidgeCV    0.251966   0.054067    750  0.245411  \n",
      ".layer1.Bottleneck2_srp    RidgeCV    0.252596   0.056952   5893  0.272297  \n",
      ".layer2.Bottleneck0_pca750 RidgeCV    0.242231   0.060241    750  0.247597  \n",
      ".layer2.Bottleneck0_srp    RidgeCV     0.24947   0.064385   5893  0.273537  \n",
      ".layer2.Bottleneck1_pca750 RidgeCV    0.248615   0.071579    750  0.244123  \n",
      ".layer2.Bottleneck1_srp    RidgeCV    0.273215   0.070333   5893  0.277309  \n",
      ".layer2.Bottleneck2_pca750 RidgeCV    0.270864   0.071686    750  0.245555  \n",
      ".layer2.Bottleneck2_srp    RidgeCV    0.287047   0.067916   5893  0.278301  \n",
      ".layer2.Bottleneck3_pca750 RidgeCV    0.287126   0.073794    750  0.248628  \n",
      ".layer2.Bottleneck3_srp    RidgeCV    0.290096   0.065982   5893  0.281601  \n",
      ".layer3.Bottleneck0_pca750 RidgeCV    0.279067   0.091324    750  0.243912  \n",
      ".layer3.Bottleneck0_srp    RidgeCV    0.280778   0.083486   5893  0.281929  \n",
      ".layer3.Bottleneck1_pca750 RidgeCV    0.268806   0.092023    750   0.24525  \n",
      ".layer3.Bottleneck1_srp    RidgeCV    0.290834   0.091229   5893  0.276147  \n",
      ".layer3.Bottleneck2_pca750 RidgeCV    0.272168   0.095258    750  0.244313  \n",
      ".layer3.Bottleneck2_srp    RidgeCV    0.301501   0.092552   5893  0.278001  \n",
      ".layer3.Bottleneck3_pca750 RidgeCV     0.28342   0.097232    750  0.247223  \n",
      ".layer3.Bottleneck3_srp    RidgeCV    0.307011   0.091488   5893  0.281384  \n",
      ".layer3.Bottleneck4_pca750 RidgeCV    0.278172   0.102368    750  0.246835  \n",
      ".layer3.Bottleneck4_srp    RidgeCV    0.311388   0.102977   5893  0.283027  \n",
      ".layer3.Bottleneck5_pca750 RidgeCV    0.308087   0.107742    750  0.245337  \n",
      ".layer3.Bottleneck5_srp    RidgeCV    0.327503   0.103923   5893  0.280373  \n",
      ".layer4.Bottleneck0_pca750 RidgeCV    0.327178   0.129214    750  0.244998  \n",
      ".layer4.Bottleneck0_srp    RidgeCV     0.33174   0.126249   5893   0.27923  \n",
      ".layer4.Bottleneck1_pca750 RidgeCV    0.312374   0.131347    750  0.244365  \n",
      ".layer4.Bottleneck1_srp    RidgeCV    0.330109   0.131654   5893  0.277981  \n",
      ".layer4.Bottleneck2_pca750 RidgeCV    0.302919   0.141669    750  0.240212  \n",
      ".layer4.Bottleneck2_srp    RidgeCV     0.30949   0.139919   5893  0.278495  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/Github/Closed-loop-visual-insilico/neural_regress/regress_eval_lib.py:125: RuntimeWarning: Mean of empty slice.\n",
      "  {\"train_score\": D2_per_unit_train_masked.mean(), \"test_score\": D2_per_unit_test_masked.mean(), }\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "batch_size = 96\n",
    "\n",
    "model_names = [\n",
    "    \"resnet50_robust\",\n",
    "    # \"resnet50\",\n",
    "    # \"resnet50_clip\",\n",
    "    # \"resnet50_dino\",\n",
    "    # \"clipag_vitb32\",\n",
    "    # \"siglip2_vitb16\",\n",
    "    # \"dinov2_vitb14_reg\",\n",
    "    # \"radio_v2.5-b\",\n",
    "]\n",
    "\n",
    "outputroot = r\"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Encoding_models\"\n",
    "output_name = \"model_outputs\"\n",
    "figdir = join(outputroot, subject_id, output_name)\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "for modelname in model_names:\n",
    "    print(f\"\\nTesting model: {modelname}\")\n",
    "    try:\n",
    "        # Load model and transforms\n",
    "        model, transforms_pipeline = load_model_transform(modelname, device)\n",
    "        print(f\"Successfully loaded model: {modelname}\")\n",
    "        dataset = ImagePathDataset(image_fps, scores=None, transform=transforms_pipeline)\n",
    "        print(f\"Successfully created dataset for {modelname} with {len(dataset)} images\")\n",
    "        # Create feature fetcher\n",
    "        fetcher = featureFetcher(model, input_size=(3, 224, 224), print_module=False)\n",
    "        print(f\"Successfully created feature fetcher for {modelname}\")\n",
    "        # Get all module names\n",
    "        all_module_names = list(fetcher.module_names.values())\n",
    "        print(f\"Total number of modules: {len(all_module_names)}\")\n",
    "        # Get layer filter for this model\n",
    "        layer_filter = MODEL_LAYER_FILTERS[modelname]\n",
    "        layer_abbrev = LAYER_ABBREVIATION_MAPS[modelname]\n",
    "        # Count layers that pass the filter\n",
    "        module_names2record = [name for name in all_module_names if layer_filter(name)]\n",
    "        print(f\"Number of layers passing filter: {len(module_names2record)}\")\n",
    "        print(f\"Filtered layer names: {module_names2record}\")\n",
    "        module_names2record_abbrev = [layer_abbrev(name) for name in module_names2record]\n",
    "        print(f\"Filtered layer names (abbreviated): {module_names2record_abbrev}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {modelname}: {str(e)}\")\n",
    "    \n",
    "    # # hook the layers\n",
    "    for name in module_names2record: \n",
    "        fetcher.record(name, store_device='cpu', ingraph=False, )\n",
    "    \n",
    "    # Record features\n",
    "    feat_dict_lyrswp = record_features(model, fetcher, dataset, batch_size=batch_size, device=device)\n",
    "    # Cleanup\n",
    "    print(f\"{modelname} done!!!\")\n",
    "    \n",
    "    fetcher.cleanup()\n",
    "    resp_mat_sel = resp_mat[:, :] # Select all channels, no mask\n",
    "    print(f\"Fitting models for All channels N={resp_mat_sel.shape[1]}\")\n",
    "    Xdict_lyrswp, Xtfmer_lyrswp = transform_features2Xdict(feat_dict_lyrswp, module_names2record, \n",
    "                            dimred_list=[\"pca750\", \"srp\", ],  #  \"srp\"\n",
    "                            pretrained_Xtransforms={}, use_pca_dual=True, use_srp_torch=True, train_split_idx=train_idx)\n",
    "    regressors = [RidgeCV(alphas=[1E-4, 1E-3, 1E-2, 1E-1, 1, 10, 100, 1E3, 1E4, 1E5, 1E6, 1E7, 1E8, 1E9], \n",
    "                        alpha_per_target=True,),\n",
    "                # MultiTaskLassoCV(cv=5, n_alphas=100, n_jobs=-1, max_iter=10000, tol=1E-4), \n",
    "                # MultiOutputSeparateLassoCV(cv=5, n_alphas=100, n_jobs=-1, max_iter=10000, tol=1E-4), \n",
    "                ] \n",
    "    regressor_names = [\"RidgeCV\"]\n",
    "    result_df_lyrswp, fit_models_lyrswp = sweep_regressors(Xdict_lyrswp, resp_mat_sel, regressors, regressor_names, \n",
    "                                                        verbose=True)\n",
    "    pred_D2_dict = compute_pred_dict_D2_per_unit(fit_models_lyrswp, Xdict_lyrswp, resp_mat_sel)\n",
    "    pkl.dump(pred_D2_dict, open(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_pred_meta.pkl\"), \"wb\"))\n",
    "    result_df_lyrswp.to_csv(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_sweep_RidgeCV.csv\"))\n",
    "    th.save(fit_models_lyrswp, join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_fitmodels_RidgeCV.pth\")) \n",
    "    # th.save(Xtfmer_lyrswp, join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_Xtfmer_RidgeCV.pth\"))\n",
    "    Xtfmer_lyrswp_sparse = {k: v for k, v in Xtfmer_lyrswp.items() if \"layer4\" in k and \"pca\" in k}\n",
    "    th.save(Xtfmer_lyrswp_sparse, join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_Xtfmer_RidgeCV.pth\"))\n",
    "    # pkl.dump(Xtfmer_lyrswp, open(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_Xtfmer_RidgeCV.pkl\"), \"wb\"))\n",
    "    # %%\n",
    "    figh = plot_result_df_per_layer(result_df_lyrswp, )\n",
    "    figh.suptitle(f\"{subject_id} {modelname} layer sweep\")\n",
    "    figh.tight_layout()\n",
    "    saveallforms(figdir, f\"{subject_id}_{modelname}_layer_sweep_synopisis\", figh=figh)\n",
    "    # %%\n",
    "    # Mask out unreliable channels and plot again\n",
    "    for thresh in [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "        channel_count = (reliability > thresh).sum()\n",
    "        result_df_masked = construct_result_df_masked(pred_D2_dict['D2_per_unit_train_dict'], \n",
    "                                                    pred_D2_dict['D2_per_unit_test_dict'], \n",
    "                                                    mask=reliability > thresh)\n",
    "        figh = plot_result_df_per_layer(result_df_masked, )\n",
    "        figh.suptitle(f\"{subject_id} {modelname} layer sweep | reliable channels > {thresh} (N={channel_count})\")\n",
    "        figh.tight_layout()\n",
    "        figh.show()\n",
    "        saveallforms(figdir, f\"{subject_id}_{modelname}_layer_sweep_synopisis_reliable_thresh{thresh}_masked\", figh=figh)\n",
    "    plt.close(\"all\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.layer1.Bottleneck0_pca750',\n",
       " '.layer1.Bottleneck0_srp',\n",
       " '.layer1.Bottleneck1_pca750',\n",
       " '.layer1.Bottleneck1_srp',\n",
       " '.layer1.Bottleneck2_pca750',\n",
       " '.layer1.Bottleneck2_srp',\n",
       " '.layer2.Bottleneck0_pca750',\n",
       " '.layer2.Bottleneck0_srp',\n",
       " '.layer2.Bottleneck1_pca750',\n",
       " '.layer2.Bottleneck1_srp',\n",
       " '.layer2.Bottleneck2_pca750',\n",
       " '.layer2.Bottleneck2_srp',\n",
       " '.layer2.Bottleneck3_pca750',\n",
       " '.layer2.Bottleneck3_srp',\n",
       " '.layer3.Bottleneck0_pca750',\n",
       " '.layer3.Bottleneck0_srp',\n",
       " '.layer3.Bottleneck1_pca750',\n",
       " '.layer3.Bottleneck1_srp',\n",
       " '.layer3.Bottleneck2_pca750',\n",
       " '.layer3.Bottleneck2_srp',\n",
       " '.layer3.Bottleneck3_pca750',\n",
       " '.layer3.Bottleneck3_srp',\n",
       " '.layer3.Bottleneck4_pca750',\n",
       " '.layer3.Bottleneck4_srp',\n",
       " '.layer3.Bottleneck5_pca750',\n",
       " '.layer3.Bottleneck5_srp',\n",
       " '.layer4.Bottleneck0_pca750',\n",
       " '.layer4.Bottleneck0_srp',\n",
       " '.layer4.Bottleneck1_pca750',\n",
       " '.layer4.Bottleneck1_srp',\n",
       " '.layer4.Bottleneck2_pca750',\n",
       " '.layer4.Bottleneck2_srp']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Xtfmer_lyrswp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtfmer_lyrswp_sparse = {k: v for k, v in Xtfmer_lyrswp.items() if \"layer4\" in k and \"pca\" in k}\n",
    "th.save(Xtfmer_lyrswp_sparse, join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_Xtfmer_RidgeCV.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(Xtfmer_lyrswp, open(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_Xtfmer_RidgeCV.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
