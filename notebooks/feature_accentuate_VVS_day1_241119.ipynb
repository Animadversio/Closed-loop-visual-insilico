{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/Closed-loop-visual-insilico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: skimage.transform is not available. Will use scipy.misc.imresize instead.\n",
      "Warning: skimage.transform is not available. Will use scipy.misc.imresize instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/Closed-loop-visual-insilico\")\n",
    "import timm\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm.auto import tqdm\n",
    "from os.path import join\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from horama import maco, plot_maco\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Normalize, Resize\n",
    "from torchvision.models import resnet50\n",
    "from circuit_toolkit.CNN_scorers import TorchScorer\n",
    "from circuit_toolkit.GAN_utils import upconvGAN, Caffenet\n",
    "from circuit_toolkit.plot_utils import to_imgrid, show_imgrid, save_imgrid, saveallforms\n",
    "from circuit_toolkit.layer_hook_utils import featureFetcher_module, featureFetcher, get_module_names\n",
    "from circuit_toolkit.dataset_utils import ImagePathDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from neural_regress.regress_lib import sweep_regressors, perform_regression_sweeplayer_RidgeCV, perform_regression_sweeplayer, record_features\n",
    "from neural_regress.sklearn_torchify_lib import SRP_torch, PCA_torch, LinearRegression_torch, SpatialAvg_torch, LinearLayer_from_sklearn\n",
    "from core.data_utils import load_neural_data, load_from_hdf5, load_neural_trial_resp_tensor, create_response_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_transform(modelname, device=\"cuda\"):\n",
    "    # Prepare model and transforms\n",
    "    if modelname == \"resnet50_robust\":\n",
    "        model = resnet50(pretrained=False)\n",
    "        model.load_state_dict(th.load(\"/n/home12/binxuwang/Github/Closed-loop-visual-insilico/checkpoints/imagenet_linf_8_pure.pt\"))\n",
    "        transforms_pipeline = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize((224, 224)),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif modelname == \"resnet50\":\n",
    "        model = resnet50(pretrained=True)\n",
    "        transforms_pipeline = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize((224, 224)),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif modelname == \"resnet50_clip\":\n",
    "        import clip\n",
    "        model_clip, preprocess = clip.load('RN50', device=device)\n",
    "        model = model_clip.visual\n",
    "        transforms_pipeline = preprocess\n",
    "    elif modelname == \"resnet50_dino\":\n",
    "        # https://github.com/facebookresearch/dino\n",
    "        model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "        transforms_pipeline = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize((224, 224)),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {modelname}\")\n",
    "        # model = timm.create_model(modelname, pretrained=True).to(device).eval()\n",
    "        # data_config = timm.data.resolve_model_data_config(model)\n",
    "        # transforms_pipeline = timm.data.create_transform(**data_config, is_training=False)\n",
    "    model = model.to(device).eval()\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    return model, transforms_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id =  'paul_20241119' \n",
    "modelname = \"resnet50_robust\"\n",
    "dataroot = r\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Projects/VVS_Accentuation\"\n",
    "figdir = join(dataroot, \"model_outputs\", subject_id, )\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "fit_models_lyrswp_RidgeCV = th.load(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_highreliab_layers_fitmodels_RidgeCV.pth\"))\n",
    "Xtfmer_lyrswp_RidgeCV = th.load(open(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_highreliab_layers_Xtfmer_RidgeCV.pkl\"), \"rb\"))\n",
    "result_df_lyrswp_RidgeCV = pd.read_csv(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_highreliab_layers_sweep_RidgeCV.csv\"))\n",
    "pred_data = pkl.load(open(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_pred_meta.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>alpha</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>n_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.layer4.Bottleneck0_srp</td>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>[1.e+05 1.e+05 1.e+09 1.e+05 1.e+05 1.e+04 1.e...</td>\n",
       "      <td>0.278677</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>5944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.layer4.Bottleneck0_pca1000</td>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>[1.e+05 1.e+05 1.e+09 1.e+04 1.e+06 1.e+04 1.e...</td>\n",
       "      <td>0.309205</td>\n",
       "      <td>0.075574</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.layer4.Bottleneck1_srp</td>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>[1.e+05 1.e+05 1.e+06 1.e+05 1.e+06 1.e+04 1.e...</td>\n",
       "      <td>0.256335</td>\n",
       "      <td>0.068880</td>\n",
       "      <td>5944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.layer4.Bottleneck1_pca1000</td>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>[1.e+05 1.e+05 1.e+07 1.e+05 1.e+06 1.e+04 1.e...</td>\n",
       "      <td>0.299131</td>\n",
       "      <td>0.076505</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.layer4.Bottleneck2_srp</td>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>[1.e+05 1.e+05 1.e+06 1.e+05 1.e+06 1.e+05 1.e...</td>\n",
       "      <td>0.255628</td>\n",
       "      <td>0.071593</td>\n",
       "      <td>5944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.layer4.Bottleneck2_pca1000</td>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>[1.e+05 1.e+05 1.e+06 1.e+05 1.e+06 1.e+05 1.e...</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>0.071705</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unnamed: 0 Unnamed: 1  \\\n",
       "0      .layer4.Bottleneck0_srp    RidgeCV   \n",
       "1  .layer4.Bottleneck0_pca1000    RidgeCV   \n",
       "2      .layer4.Bottleneck1_srp    RidgeCV   \n",
       "3  .layer4.Bottleneck1_pca1000    RidgeCV   \n",
       "4      .layer4.Bottleneck2_srp    RidgeCV   \n",
       "5  .layer4.Bottleneck2_pca1000    RidgeCV   \n",
       "\n",
       "                                               alpha  train_score  test_score  \\\n",
       "0  [1.e+05 1.e+05 1.e+09 1.e+05 1.e+05 1.e+04 1.e...     0.278677    0.065491   \n",
       "1  [1.e+05 1.e+05 1.e+09 1.e+04 1.e+06 1.e+04 1.e...     0.309205    0.075574   \n",
       "2  [1.e+05 1.e+05 1.e+06 1.e+05 1.e+06 1.e+04 1.e...     0.256335    0.068880   \n",
       "3  [1.e+05 1.e+05 1.e+07 1.e+05 1.e+06 1.e+04 1.e...     0.299131    0.076505   \n",
       "4  [1.e+05 1.e+05 1.e+06 1.e+05 1.e+06 1.e+05 1.e...     0.255628    0.071593   \n",
       "5  [1.e+05 1.e+05 1.e+06 1.e+05 1.e+06 1.e+05 1.e...     0.276667    0.071705   \n",
       "\n",
       "   n_feat  \n",
       "0    5944  \n",
       "1    1000  \n",
       "2    5944  \n",
       "3    1000  \n",
       "4    5944  \n",
       "5    1000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_lyrswp_RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = r\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Projects/VVS_Accentuation\"\n",
    "stimroot = join(dataroot, \"shared1000\")\n",
    "data_path = join(dataroot, \"vvs-accentuate-day1_normalize_paul_241119.h5\")\n",
    "subject_id =  'paul_20241119' \n",
    "# Load data\n",
    "data_dict = load_neural_data(data_path, subject_id, stimroot)\n",
    "image_fps = data_dict['image_fps']\n",
    "resp_mat = data_dict['resp_mat']\n",
    "reliability = data_dict['reliability']\n",
    "ncsnr = data_dict['ncsnr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 out of 64 channels\n",
      "('.layer4.Bottleneck0_srp', 'RidgeCV') 0.24510798994714875\n",
      "('.layer4.Bottleneck0_pca1000', 'RidgeCV') 0.28037262488102976\n",
      "('.layer4.Bottleneck1_srp', 'RidgeCV') 0.24794823075047642\n",
      "('.layer4.Bottleneck1_pca1000', 'RidgeCV') 0.2801949067450697\n",
      "('.layer4.Bottleneck2_srp', 'RidgeCV') 0.2686220619945388\n",
      "('.layer4.Bottleneck2_pca1000', 'RidgeCV') 0.2682572684280461\n"
     ]
    }
   ],
   "source": [
    "chan_msk = ncsnr > 0.8\n",
    "print(f\"Using {chan_msk.sum()} out of {chan_msk.shape[0]} channels\")\n",
    "for k, v in pred_data[\"D2_per_unit_test_dict\"].items():\n",
    "    print(k, v[chan_msk].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ('.layer4.Bottleneck1_pca1000', 'RidgeCV')\n",
    "readout_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_readout_RidgeCV_{key[0]}.pth\")\n",
    "Xtransform_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_Xtfmer_RidgeCV_{key[0]}.pkl\")\n",
    "meta_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_meta_RidgeCV_{key[0]}.pkl\")\n",
    "\n",
    "regressor = fit_models_lyrswp_RidgeCV[key]\n",
    "Xtfmer = Xtfmer_lyrswp_RidgeCV[key[0]]\n",
    "pred_rsp = pred_data[\"pred_dict\"][key]\n",
    "D2_per_unit_test = pred_data[\"D2_per_unit_test_dict\"][key]\n",
    "D2_per_unit_train = pred_data[\"D2_per_unit_train_dict\"][key]\n",
    "\n",
    "readout = LinearLayer_from_sklearn(regressor)\n",
    "th.save(readout.state_dict(), readout_path)\n",
    "pkl.dump(Xtfmer, open(Xtransform_path, \"wb\"))\n",
    "th.save({\n",
    "    \"reliability\": reliability,\n",
    "    \"ncsnr\": ncsnr,\n",
    "    \"D2_per_unit_test\": D2_per_unit_test,\n",
    "    \"D2_per_unit_train\": D2_per_unit_train\n",
    "}, meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: resnet50_robust\n",
      "Using 12 out of 64 channels\n",
      "('.layer4.Bottleneck0_srp', 'RidgeCV') 0.24510798994714875\n",
      "('.layer4.Bottleneck0_pca1000', 'RidgeCV') 0.28037262488102976\n",
      "('.layer4.Bottleneck1_srp', 'RidgeCV') 0.24794823075047642\n",
      "('.layer4.Bottleneck1_pca1000', 'RidgeCV') 0.2801949067450697\n",
      "('.layer4.Bottleneck2_srp', 'RidgeCV') 0.2686220619945388\n",
      "('.layer4.Bottleneck2_pca1000', 'RidgeCV') 0.2682572684280461\n",
      "Model: resnet50\n",
      "Using 12 out of 64 channels\n",
      "('.layer4.Bottleneck0_srp', 'RidgeCV') 0.2636292499760762\n",
      "('.layer4.Bottleneck0_pca1000', 'RidgeCV') 0.2733498696896721\n",
      "('.layer4.Bottleneck1_srp', 'RidgeCV') 0.2549499686043723\n",
      "('.layer4.Bottleneck1_pca1000', 'RidgeCV') 0.26240913071028354\n",
      "('.layer4.Bottleneck2_srp', 'RidgeCV') 0.22853612133746357\n",
      "('.layer4.Bottleneck2_pca1000', 'RidgeCV') 0.23705998307581821\n",
      "Model: resnet50_clip\n",
      "Using 12 out of 64 channels\n",
      "('.layer4.Bottleneck0_srp', 'RidgeCV') 0.26087259310950367\n",
      "('.layer4.Bottleneck0_pca1000', 'RidgeCV') 0.2892533977742961\n",
      "('.layer4.Bottleneck1_srp', 'RidgeCV') 0.24340144139270636\n",
      "('.layer4.Bottleneck1_pca1000', 'RidgeCV') 0.27133534475497756\n",
      "('.layer4.Bottleneck2_srp', 'RidgeCV') 0.25728105554129505\n",
      "('.layer4.Bottleneck2_pca1000', 'RidgeCV') 0.2593398769468515\n",
      "Model: resnet50_dino\n",
      "Using 12 out of 64 channels\n",
      "('.layer4.Bottleneck0_srp', 'RidgeCV') 0.2774060877726175\n",
      "('.layer4.Bottleneck0_pca1000', 'RidgeCV') 0.30068763193242204\n",
      "('.layer4.Bottleneck1_srp', 'RidgeCV') 0.25919443167371825\n",
      "('.layer4.Bottleneck1_pca1000', 'RidgeCV') 0.29802214528575727\n",
      "('.layer4.Bottleneck2_srp', 'RidgeCV') 0.238738232215957\n",
      "('.layer4.Bottleneck2_pca1000', 'RidgeCV') 0.26708993642201245\n"
     ]
    }
   ],
   "source": [
    "dataroot = r\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Projects/VVS_Accentuation\"\n",
    "stimroot = join(dataroot, \"shared1000\")\n",
    "data_path = join(dataroot, \"vvs-accentuate-day1_normalize_paul_241119.h5\")\n",
    "subject_id =  'paul_20241119' \n",
    "# Load data\n",
    "\n",
    "for modelname in [\"resnet50_robust\", \"resnet50\", \"resnet50_clip\", \"resnet50_dino\"]:\n",
    "    figdir = join(dataroot, \"model_outputs\", subject_id, )\n",
    "\n",
    "    fit_models_lyrswp_RidgeCV = th.load(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_highreliab_layers_fitmodels_RidgeCV.pth\"))\n",
    "    Xtfmer_lyrswp_RidgeCV = th.load(open(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_highreliab_layers_Xtfmer_RidgeCV.pkl\"), \"rb\"))\n",
    "    result_df_lyrswp_RidgeCV = pd.read_csv(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_highreliab_layers_sweep_RidgeCV.csv\"))\n",
    "    pred_data = pkl.load(open(join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_layers_pred_meta.pkl\"), \"rb\"))\n",
    "    data_dict = load_neural_data(data_path, subject_id, stimroot)\n",
    "\n",
    "    image_fps = data_dict['image_fps']\n",
    "    resp_mat = data_dict['resp_mat']\n",
    "    reliability = data_dict['reliability']\n",
    "    ncsnr = data_dict['ncsnr']\n",
    "\n",
    "    print(f\"Model: {modelname}\")\n",
    "    chan_msk = ncsnr > 0.8\n",
    "    print(f\"Using {chan_msk.sum()} out of {chan_msk.shape[0]} channels\")\n",
    "    for k, v in pred_data[\"D2_per_unit_test_dict\"].items():\n",
    "        print(k, v[chan_msk].mean())\n",
    "\n",
    "    key = ('.layer4.Bottleneck1_pca1000', 'RidgeCV')\n",
    "    # export paths \n",
    "    readout_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_readout_RidgeCV_{key[0]}.pth\")\n",
    "    Xtransform_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_Xtfmer_RidgeCV_{key[0]}.pkl\")\n",
    "    meta_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_meta_RidgeCV_{key[0]}.pkl\")\n",
    "\n",
    "    regressor = fit_models_lyrswp_RidgeCV[key]\n",
    "    Xtfmer = Xtfmer_lyrswp_RidgeCV[key[0]]\n",
    "    pred_rsp = pred_data[\"pred_dict\"][key]\n",
    "    D2_per_unit_test = pred_data[\"D2_per_unit_test_dict\"][key]\n",
    "    D2_per_unit_train = pred_data[\"D2_per_unit_train_dict\"][key]\n",
    "\n",
    "    readout = LinearLayer_from_sklearn(regressor)\n",
    "    th.save(readout.state_dict(), readout_path)\n",
    "    pkl.dump(Xtfmer, open(Xtransform_path, \"wb\"))\n",
    "    th.save({\n",
    "        \"reliability\": reliability,\n",
    "        \"ncsnr\": ncsnr,\n",
    "        \"D2_per_unit_test\": D2_per_unit_test,\n",
    "        \"D2_per_unit_train\": D2_per_unit_train\n",
    "    }, meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Projects/VVS_Accentuation/model_outputs/paul_20241119/paul_20241119_resnet50_robust_sweep_regressors_readout_RidgeCV_.layer4.Bottleneck1_pca1000.pth'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readout_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Xtransform_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_Xtfmer_RidgeCV_{key[0]}.pkl\")\n",
    "Xtransform_path = '/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Projects/VVS_Accentuation/model_outputs/paul_20241119/paul_20241119_resnet50_robust_sweep_regressors_Xtfmer_RidgeCV_.layer4.Bottleneck1_pca1000.pkl'\n",
    "# readout_path = join(figdir, f\"{subject_id}_{modelname}_sweep_regressors_readout_RidgeCV_{key[0]}.pth\")\n",
    "readout_path = '/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Projects/VVS_Accentuation/model_outputs/paul_20241119/paul_20241119_resnet50_robust_sweep_regressors_readout_RidgeCV_.layer4.Bottleneck1_pca1000.pth' \n",
    "device = \"cuda\"\n",
    "layer_name = \".layer4.Bottleneck1\"  #key[0]\n",
    "model, transforms_pipeline = load_model_transform(modelname, device=device)\n",
    "model = model.eval().to(device)\n",
    "model.requires_grad_(False)\n",
    "fetcher = featureFetcher(model, input_size=(3, 224, 224), print_module=False)\n",
    "fetcher.record(layer_name,  ingraph=True, store_device=device)\n",
    "# # Define the readout layer and load weights\n",
    "state_dict = th.load(readout_path)\n",
    "readout = nn.Linear(state_dict['weight'].shape[1], state_dict['weight'].shape[0], bias=True).cuda()\n",
    "readout.load_state_dict(state_dict)\n",
    "pca = pkl.load(open(Xtransform_path, \"rb\"))\n",
    "Xtransform = PCA_torch(pca, device=device)\n",
    "# Define the prediction pipeline\n",
    "def objective(images):\n",
    "    # Forward pass through the feature extractor\n",
    "    model(images)\n",
    "    feat_tsr = fetcher[layer_name]  # Access the layer4 feature\n",
    "    feat_vec = Xtransform(feat_tsr)\n",
    "    return readout(feat_vec).mean(dim=1).mean()\n",
    "def objective_unit(images):\n",
    "    # Forward pass through the feature extractor\n",
    "    model(images)\n",
    "    feat_tsr = fetcher[layer_name]  # Access the layer4 feature\n",
    "    feat_vec = Xtransform(feat_tsr)\n",
    "    return readout(feat_vec)[:, 1].mean()\n",
    "def check_gradient(objective_fn):\n",
    "    \"\"\"Check if gradients can flow through the objective function.\"\"\"\n",
    "    img_opt = th.randn(1, 3, 224, 224).cuda()\n",
    "    img_opt.requires_grad_(True)\n",
    "    resp = objective_fn(img_opt)\n",
    "    resp.mean().backward()\n",
    "    print(resp.shape)\n",
    "    assert img_opt.grad is not None\n",
    "check_gradient(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/Projects/VVS_Accentuation/model_outputs/paul_20241119\n"
     ]
    }
   ],
   "source": [
    "!echo {figdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paul_20241119_resnet50_clip_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.pdf\n",
      "paul_20241119_resnet50_clip_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.png\n",
      "paul_20241119_resnet50_clip_layer_sweep_GridCV_synopisis.pdf\n",
      "paul_20241119_resnet50_clip_layer_sweep_GridCV_synopisis.png\n",
      "paul_20241119_resnet50_clip_sweep_regressors_highreliab_layers_fitmodels_RidgeCV.pth\n",
      "paul_20241119_resnet50_clip_sweep_regressors_highreliab_layers_sweep_RidgeCV.csv\n",
      "paul_20241119_resnet50_clip_sweep_regressors_highreliab_layers_sweep_RidgeCV_formatted.csv\n",
      "paul_20241119_resnet50_clip_sweep_regressors_highreliab_layers_Xtfmer_RidgeCV.pkl\n",
      "paul_20241119_resnet50_clip_sweep_regressors_layers_pred_meta.pkl\n",
      "paul_20241119_resnet50_dino_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.pdf\n",
      "paul_20241119_resnet50_dino_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.png\n",
      "paul_20241119_resnet50_dino_layer_sweep_GridCV_synopisis.pdf\n",
      "paul_20241119_resnet50_dino_layer_sweep_GridCV_synopisis.png\n",
      "paul_20241119_resnet50_dino_sweep_regressors_highreliab_layers_fitmodels_RidgeCV.pth\n",
      "paul_20241119_resnet50_dino_sweep_regressors_highreliab_layers_sweep_RidgeCV.csv\n",
      "paul_20241119_resnet50_dino_sweep_regressors_highreliab_layers_sweep_RidgeCV_formatted.csv\n",
      "paul_20241119_resnet50_dino_sweep_regressors_highreliab_layers_Xtfmer_RidgeCV.pkl\n",
      "paul_20241119_resnet50_dino_sweep_regressors_layers_pred_meta.pkl\n",
      "paul_20241119_resnet50_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.pdf\n",
      "paul_20241119_resnet50_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.png\n",
      "paul_20241119_resnet50_layer_sweep_GridCV_synopisis.pdf\n",
      "paul_20241119_resnet50_layer_sweep_GridCV_synopisis.png\n",
      "paul_20241119_resnet50_robust_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.pdf\n",
      "paul_20241119_resnet50_robust_layer_sweep_GridCV_synopisis_D2perunit_ncsnr_scatter.png\n",
      "paul_20241119_resnet50_robust_layer_sweep_GridCV_synopisis.pdf\n",
      "paul_20241119_resnet50_robust_layer_sweep_GridCV_synopisis.png\n",
      "paul_20241119_resnet50_robust_sweep_regressors_highreliab_layers_fitmodels_RidgeCV.pth\n",
      "paul_20241119_resnet50_robust_sweep_regressors_highreliab_layers_sweep_RidgeCV.csv\n",
      "paul_20241119_resnet50_robust_sweep_regressors_highreliab_layers_sweep_RidgeCV_formatted.csv\n",
      "paul_20241119_resnet50_robust_sweep_regressors_highreliab_layers_Xtfmer_RidgeCV.pkl\n",
      "paul_20241119_resnet50_robust_sweep_regressors_layers_pred_meta.pkl\n",
      "paul_20241119_resnet50_robust_sweep_regressors_meta_RidgeCV_.layer4.Bottleneck1_pca1000.pkl\n",
      "paul_20241119_resnet50_robust_sweep_regressors_readout_RidgeCV_.layer4.Bottleneck1_pca1000.pth\n",
      "paul_20241119_resnet50_robust_sweep_regressors_Xtfmer_RidgeCV_.layer4.Bottleneck1_pca1000.pkl\n",
      "paul_20241119_resnet50_sweep_regressors_highreliab_layers_fitmodels_RidgeCV.pth\n",
      "paul_20241119_resnet50_sweep_regressors_highreliab_layers_sweep_RidgeCV.csv\n",
      "paul_20241119_resnet50_sweep_regressors_highreliab_layers_sweep_RidgeCV_formatted.csv\n",
      "paul_20241119_resnet50_sweep_regressors_highreliab_layers_Xtfmer_RidgeCV.pkl\n",
      "paul_20241119_resnet50_sweep_regressors_layers_pred_meta.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls {figdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;PCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(n_components=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "layer_name = \".layer4.Bottleneck1\"  #key[0]\n",
    "model, transforms_pipeline = load_model_transform(modelname, device=device)\n",
    "fetcher = featureFetcher(model, input_size=(3, 224, 224), print_module=False)\n",
    "fetcher.record(layer_name,  ingraph=True, store_device=device)\n",
    "# # Define the readout layer and load weights\n",
    "state_dict = th.load(readout_path)\n",
    "readout = nn.Linear(state_dict['weight'].shape[1], state_dict['weight'].shape[0], bias=True).cuda()\n",
    "readout.load_state_dict(state_dict)\n",
    "pca = pkl.load(open(Xtransform_path, \"rb\"))\n",
    "Xtransform = PCA_torch(pca, device=device)\n",
    "# Define the prediction pipeline\n",
    "def objective(images):\n",
    "    # Forward pass through the feature extractor\n",
    "    model(images)\n",
    "    feat_tsr = fetcher[layer_name]  # Access the layer4 feature\n",
    "    feat_vec = Xtransform(feat_tsr)\n",
    "    return readout(feat_vec).mean(dim=1).mean()\n",
    "\n",
    "check_gradient(objective)\n",
    "# Optimization part 1: MACO\n",
    "image1, alpha1 = maco(objective, **maco_hyperparams, device='cuda')\n",
    "plot_maco(image1, alpha1)\n",
    "saveallforms(figdir, f\"{subject_id}_{modelname}_{layer_name}_{RD_method}_MACO_population_mean\")\n",
    "plt.show()\n",
    "\n",
    "img_col = []\n",
    "for unit_id in range(len(D2)):\n",
    "    print(f\"Optimizing unit {unit_id}\")\n",
    "    print(f\"D^2: {D2[unit_id]}\")\n",
    "\n",
    "    def unit_objective(images):\n",
    "        # Forward pass through the feature extractor\n",
    "        model(images)\n",
    "        feat_tsr = fetcher[layer_name]  # Access the layer4 feature\n",
    "        feat_vec = Xtransform(feat_tsr)\n",
    "        return readout(feat_vec)[:, unit_id].mean()\n",
    "    \n",
    "    image_unit, alpha_unit = maco(unit_objective, **maco_hyperparams, device='cuda')\n",
    "    img_col.append((image_unit, alpha_unit))\n",
    "    plot_maco(image_unit, alpha_unit)\n",
    "\n",
    "visualize_results(img_col, D2, )\n",
    "plt.suptitle(f\"{subject_id} {modelname} {layer_name} {RD_method} MACO unit compilation\")\n",
    "saveallforms(figdir, f\"{subject_id}_{modelname}_{layer_name}_{RD_method}_MACO_unit_cmp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
