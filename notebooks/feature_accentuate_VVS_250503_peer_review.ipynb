{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/Closed-loop-visual-insilico\")\n",
    "import timm\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from os.path import join\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from horama import maco, plot_maco\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Normalize, Resize\n",
    "from circuit_toolkit.CNN_scorers import TorchScorer\n",
    "from circuit_toolkit.GAN_utils import upconvGAN, Caffenet\n",
    "from circuit_toolkit.plot_utils import to_imgrid, show_imgrid, save_imgrid, saveallforms\n",
    "from circuit_toolkit.layer_hook_utils import featureFetcher_module, featureFetcher, get_module_names\n",
    "from circuit_toolkit.dataset_utils import ImagePathDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_regress.regress_lib import sweep_regressors, perform_regression_sweeplayer_RidgeCV, perform_regression_sweeplayer, record_features\n",
    "from neural_regress.sklearn_torchify_lib import SRP_torch, PCA_torch, LinearRegression_torch, SpatialAvg_torch, LinearLayer_from_sklearn\n",
    "from core.data_utils import load_neural_data, load_from_hdf5, load_neural_trial_resp_tensor, create_response_tensor, parse_image_fullpaths\n",
    "from core.model_load_utils import load_model_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up paths for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch9_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch0_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch15_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch2_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch9_accentuation_config.yaml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_root = r\"/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs\"\n",
    "subject_id = \"red_20250428-20250430\"\n",
    "config_dir = join(config_root, subject_id)\n",
    "config_files = sorted(glob.glob(join(config_dir, \"*.yaml\")))\n",
    "\n",
    "config_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch19_accentuation_config.yaml',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch19_accentuation_config.yaml']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chan_pattern = \"_Ch19_\"\n",
    "config_pre_chan = [f for f in config_files if chan_pattern in f]\n",
    "config_pre_chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_config = yaml.safe_load(open(config_pre_chan[0]))\n",
    "xtransform_path = acc_config['xtransform_path']\n",
    "meta_path = acc_config['meta_path']\n",
    "readout_path = acc_config['readout_path']\n",
    "model_name = acc_config['model_name']\n",
    "unit_ids = acc_config['unit_ids']\n",
    "layer_name = acc_config['layer_name']\n",
    "regressor = acc_config['fit_method_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "def check_gradient(objective_fn):\n",
    "    \"\"\"Check if gradients can flow through the objective function.\"\"\"\n",
    "    img_opt = th.randn(1, 3, 224, 224).cuda()\n",
    "    img_opt.requires_grad_(True)\n",
    "    resp = objective_fn(img_opt)\n",
    "    resp.mean().backward()\n",
    "    print(resp.shape)\n",
    "    assert img_opt.grad is not None\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model, transforms_pipeline = load_model_transform(model_name, device=device)\n",
    "model = model.eval().to(device)\n",
    "model.requires_grad_(False)\n",
    "fetcher = featureFetcher(model, input_size=(3, 224, 224), print_module=False)\n",
    "fetcher.record(layer_name, ingraph=True, store_device=device)\n",
    "# Load readout layer\n",
    "readout = th.load(readout_path).to(device)\n",
    "Xtransform = th.load(xtransform_path).to(device)\n",
    "# Load PCA transform\n",
    "\n",
    "def predict_population_response(images):\n",
    "    \"\"\"Predict neural population responses for input images.\n",
    "    \n",
    "    Args:\n",
    "        images (torch.Tensor): Input images of shape (batch_size, 3, 224, 224)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Predicted neural responses\n",
    "    \"\"\"\n",
    "    model(images)\n",
    "    feat_tsr = fetcher[layer_name]\n",
    "    feat_vec = Xtransform(feat_tsr)\n",
    "    return readout(feat_vec)\n",
    "\n",
    "check_gradient(predict_population_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'AlexNet_training_seed_01',\n",
       " 'fit_method_name': 'RidgeCV',\n",
       " 'unit_ids': [19],\n",
       " 'subject_id': 'red_20250428-20250430',\n",
       " 'layer_name': '.features.MaxPool2d12',\n",
       " 'outputdir': '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430',\n",
       " 'readout_path': '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_readout_.features.MaxPool2d12_pca750_RidgeCV.pth',\n",
       " 'xtransform_path': '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_Xtfmer_.features.MaxPool2d12_pca750_RidgeCV_JITscript.pt',\n",
       " 'meta_path': '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_meta_.features.MaxPool2d12_pca750_RidgeCV.pkl',\n",
       " 'result_folder': '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_AlexNet_training_seed_01_accentuation',\n",
       " 'gifs_folder': '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_AlexNet_training_seed_01_gifs',\n",
       " 'log_dir': '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_AlexNet_training_seed_01_logs',\n",
       " 'seed_image_paths': ['shared1000/shared0575_nsd43157.png',\n",
       "  'shared1000/shared0850_nsd61798.png',\n",
       "  'shared1000/shared0968_nsd70194.png',\n",
       "  'shared1000/shared0241_nsd20065.png',\n",
       "  'shared1000/shared0160_nsd13231.png',\n",
       "  'shared1000/shared0070_nsd07008.png',\n",
       "  'shared1000/shared0055_nsd05879.png',\n",
       "  'shared1000/shared0668_nsd48623.png',\n",
       "  'shared1000/shared0488_nsd36979.png',\n",
       "  'shared1000/shared0940_nsd68312.png'],\n",
       " 'hp_tuning': {'enabled': True,\n",
       "  'num_images': 3,\n",
       "  'configs': [{'noise': 0.3,\n",
       "    'decay': 2.5,\n",
       "    'total_steps': 6000,\n",
       "    'learning_rate': 12.0,\n",
       "    'box_size': [0.9, 0.95]},\n",
       "   {'noise': 0.25,\n",
       "    'decay': 2.4,\n",
       "    'total_steps': 6000,\n",
       "    'learning_rate': 12.0,\n",
       "    'box_size': [0.9, 0.95]},\n",
       "   {'noise': 0.15,\n",
       "    'decay': 2.0,\n",
       "    'total_steps': 6000,\n",
       "    'learning_rate': 12.0,\n",
       "    'box_size': [0.9, 0.95]},\n",
       "   {'noise': 0.1,\n",
       "    'decay': 1.5,\n",
       "    'total_steps': 6000,\n",
       "    'learning_rate': 12.0,\n",
       "    'box_size': [0.9, 0.95]},\n",
       "   {'noise': 0.05,\n",
       "    'decay': 1.2,\n",
       "    'total_steps': 6000,\n",
       "    'learning_rate': 12.0,\n",
       "    'box_size': [0.9, 0.95]}]},\n",
       " 'fa_hyperparameters': {'noise': 0.1,\n",
       "  'decay': 1.5,\n",
       "  'total_steps': 6000,\n",
       "  'learning_rate': 12.0,\n",
       "  'image_size': 1024,\n",
       "  'model_input_size': 224,\n",
       "  'values_range': [0.0, 1.0],\n",
       "  'crops_per_iteration': 8,\n",
       "  'box_size': [0.9, 0.95],\n",
       "  'penalty': 0.0},\n",
       " 'extend_range': 0.25,\n",
       " 'num_levels': 11,\n",
       " 'generate_gifs': True,\n",
       " 'bicubic_interpolation': False,\n",
       " 'generate_comprehensive_gifs': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(objective_fn):\n",
    "    \"\"\"Check if gradients can flow through the objective function.\"\"\"\n",
    "    img_opt = th.randn(1, 3, 224, 224).cuda()\n",
    "    img_opt.requires_grad_(True)\n",
    "    resp = objective_fn(img_opt)\n",
    "    resp.mean().backward()\n",
    "    print(resp.shape)\n",
    "    assert img_opt.grad is not None\n",
    "\n",
    "\n",
    "def get_predictor_from_config(acc_config, device=\"cuda\"):\n",
    "    \"\"\"Create a function that predicts neural population responses for images.\n",
    "    \n",
    "    Args:\n",
    "        subject_id (str): ID of the subject\n",
    "        modelname (str): Name of the model to use (e.g. \"resnet50_robust\") \n",
    "        layer_name (str): Name of layer to extract features from\n",
    "        device (str): Device to run model on (\"cuda\" or \"cpu\")\n",
    "        \n",
    "    Returns:\n",
    "        function: A function that takes images as input and returns predicted population responses\n",
    "    \"\"\"\n",
    "    if isinstance(acc_config, str):\n",
    "        acc_config = yaml.safe_load(open(acc_config))\n",
    "        \n",
    "    # Construct paths\n",
    "    xtransform_path = acc_config['xtransform_path']\n",
    "    meta_path = acc_config['meta_path']\n",
    "    readout_path = acc_config['readout_path']\n",
    "    model_name = acc_config['model_name']\n",
    "    unit_ids = acc_config['unit_ids']\n",
    "    layer_name = acc_config['layer_name']\n",
    "    regressor = acc_config['fit_method_name']\n",
    "    print(f\"Loading model and set up feature extraction: model {model_name}, layer {layer_name}\")\n",
    "    print(f\"Target unit ids: {unit_ids}\")\n",
    "    print(f\"Loading readout layer and PCA transform: readout {readout_path}, xtransform             {xtransform_path}\")\n",
    "    # Load model and set up feature extraction\n",
    "    model, transforms_pipeline = load_model_transform(model_name, device=device)\n",
    "    model = model.eval().to(device)\n",
    "    model.requires_grad_(False)\n",
    "    fetcher = featureFetcher(model, input_size=(3, 224, 224), print_module=False)\n",
    "    fetcher.record(layer_name, ingraph=True, store_device=device)\n",
    "    # Load readout layer and PCA transform\n",
    "    readout = th.load(readout_path).to(device)\n",
    "    Xtransform = th.load(xtransform_path).to(device)\n",
    "    def predict_population_response(images):\n",
    "        \"\"\"Predict neural population responses for input images.\n",
    "        \n",
    "        Args:\n",
    "            images (torch.Tensor): Input images of shape (batch_size, 3, 224, 224)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Predicted neural responses\n",
    "        \"\"\"\n",
    "        model(images)\n",
    "        feat_tsr = fetcher[layer_name]\n",
    "        feat_vec = Xtransform(feat_tsr)\n",
    "        return readout(feat_vec)\n",
    "    \n",
    "    \n",
    "    def predict_target_unit_response(images):\n",
    "        \"\"\"Predict neural population responses for input images.\n",
    "        \n",
    "        Args:\n",
    "            images (torch.Tensor): Input images of shape (batch_size, 3, 224, 224)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Predicted neural responses\n",
    "        \"\"\"\n",
    "        model(images)\n",
    "        feat_tsr = fetcher[layer_name]\n",
    "        feat_vec = Xtransform(feat_tsr)\n",
    "        return readout(feat_vec)[:, unit_ids]\n",
    "\n",
    "    check_gradient(predict_population_response)\n",
    "    print(\"Gradient check passed!\")\n",
    "    return predict_population_response, predict_target_unit_response, \\\n",
    "           model, transforms_pipeline, fetcher, Xtransform, readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_root = r\"/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs\"\n",
    "subject_id = \"red_20250428-20250430\"\n",
    "config_dir = join(config_root, subject_id)\n",
    "config_files = sorted(glob.glob(join(config_dir, \"*.yaml\")))\n",
    "chan_pattern = \"_Ch19_\"\n",
    "config_pre_chan = [f for f in config_files if chan_pattern in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all predictors can be loaded! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model AlexNet_training_seed_01, layer .features.MaxPool2d12\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_readout_.features.MaxPool2d12_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_Xtfmer_.features.MaxPool2d12_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model clipag_vitb32, layer .transformer.resblocks.ResidualAttentionBlock9\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch19_readout_.transformer.resblocks.ResidualAttentionBlock9_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch19_Xtfmer_.transformer.resblocks.ResidualAttentionBlock9_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model dinov2_vitb14_reg, layer .blocks.NestedTensorBlock11\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch19_readout_.blocks.NestedTensorBlock11_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch19_Xtfmer_.blocks.NestedTensorBlock11_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model radio_v2.5-b, layer .model.blocks.Block10\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch19_readout_.model.blocks.Block10_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch19_Xtfmer_.model.blocks.Block10_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/NVlabs_RADIO_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model regnety_640, layer .s4.Bottleneckb1\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch19_readout_.s4.Bottleneckb1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch19_Xtfmer_.s4.Bottleneckb1_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model resnet50, layer .layer4.Bottleneck0\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch19_readout_.layer4.Bottleneck0_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch19_Xtfmer_.layer4.Bottleneck0_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model resnet50_clip, layer .layer4.Bottleneck1\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch19_readout_.layer4.Bottleneck1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch19_Xtfmer_.layer4.Bottleneck1_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model resnet50_dino, layer .layer4.Bottleneck1\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch19_readout_.layer4.Bottleneck1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch19_Xtfmer_.layer4.Bottleneck1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dino_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model resnet50_robust, layer .layer4.Bottleneck2\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch19_readout_.layer4.Bottleneck2_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_robust_Ch19_Xtfmer_.layer4.Bottleneck2_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "Loading model and set up feature extraction: model siglip2_vitb16, layer .trunk.blocks.Block9\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch19_readout_.trunk.blocks.Block9_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_siglip2_vitb16_Ch19_Xtfmer_.trunk.blocks.Block9_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "config_root = r\"/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs\"\n",
    "subject_id = \"red_20250428-20250430\"\n",
    "config_dir = join(config_root, subject_id)\n",
    "config_files = sorted(glob.glob(join(config_dir, \"*.yaml\")))\n",
    "chan_pattern = \"_Ch19_\"\n",
    "config_pre_chan = [f for f in config_files if chan_pattern in f]\n",
    "for config_file in config_pre_chan:\n",
    "    population_predictor, target_unit_predictor, \\\n",
    "        model, transforms_pipeline, fetcher, Xtransform, readout \\\n",
    "            = get_predictor_from_config(config_file, device=\"cuda\")\n",
    "    # print(population_predictor)\n",
    "    # print(target_unit_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the stimuli set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'red_20250428-20250430'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_siglip2_vitb16_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_resnet50_clip_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_resnet50_robust_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_dinov2_vitb14_reg_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_AlexNet_training_seed_01_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_resnet50_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_regnety_640_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_resnet50_dino_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_clipag_vitb32_accentuation',\n",
       " '/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs/02-05-2025_red_20250428-20250430_radio_v2.5-b_accentuation']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_stim_root = r\"/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_outputs\"\n",
    "target_subfolder = glob.glob(join(acc_stim_root, f\"*{subject_id}*_accentuation\"))\n",
    "target_subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_siglip2_vitb16_accentuation\n",
      "  - siglip2_vitb16_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.21913835406303406.png\n",
      "  - siglip2_vitb16_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0277771949768066.png\n",
      "  - siglip2_vitb16_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.1924805641174316.png\n",
      "  - siglip2_vitb16_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5727200508117676.png\n",
      "  - siglip2_vitb16_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3872941732406616.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_resnet50_clip_accentuation\n",
      "  - resnet50_clip_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.2184920310974121.png\n",
      "  - resnet50_clip_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.027352213859558.png\n",
      "  - resnet50_clip_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.836473822593689.png\n",
      "  - resnet50_clip_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5900154113769531.png\n",
      "  - resnet50_clip_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3803688287734985.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_resnet50_robust_accentuation\n",
      "  - resnet50_robust_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.219962939620018.png\n",
      "  - resnet50_robust_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0291764736175537.png\n",
      "  - resnet50_robust_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.8366221189498901.png\n",
      "  - resnet50_robust_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.589899480342865.png\n",
      "  - resnet50_robust_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3803324699401855.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_dinov2_vitb14_reg_accentuation\n",
      "  - dinov2_vitb14_reg_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.21868185698986053.png\n",
      "  - dinov2_vitb14_reg_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0282909870147705.png\n",
      "  - dinov2_vitb14_reg_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.8364901542663574.png\n",
      "  - dinov2_vitb14_reg_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5725978016853333.png\n",
      "  - dinov2_vitb14_reg_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3849091529846191.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_AlexNet_training_seed_01_accentuation\n",
      "  - AlexNet_training_seed_01_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.22128941118717194.png\n",
      "  - AlexNet_training_seed_01_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0278555154800415.png\n",
      "  - AlexNet_training_seed_01_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.8385322093963623.png\n",
      "  - AlexNet_training_seed_01_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5721354484558105.png\n",
      "  - AlexNet_training_seed_01_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.380919098854065.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_resnet50_accentuation\n",
      "  - resnet50_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.21846050024032593.png\n",
      "  - resnet50_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0284550189971924.png\n",
      "  - resnet50_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.8370397090911865.png\n",
      "  - resnet50_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5765054225921631.png\n",
      "  - resnet50_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3835681676864624.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_regnety_640_accentuation\n",
      "  - regnety_640_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.2210223376750946.png\n",
      "  - regnety_640_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0273547172546387.png\n",
      "  - regnety_640_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.8366522789001465.png\n",
      "  - regnety_640_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5744436979293823.png\n",
      "  - regnety_640_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3805406093597412.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_resnet50_dino_accentuation\n",
      "  - resnet50_dino_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.21875673532485962.png\n",
      "  - resnet50_dino_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0289300680160522.png\n",
      "  - resnet50_dino_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.83707594871521.png\n",
      "  - resnet50_dino_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.57150799036026.png\n",
      "  - resnet50_dino_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3805205821990967.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_clipag_vitb32_accentuation\n",
      "  - clipag_vitb32_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.2212853878736496.png\n",
      "  - clipag_vitb32_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0278148651123047.png\n",
      "  - clipag_vitb32_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.8379993438720703.png\n",
      "  - clipag_vitb32_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5741710662841797.png\n",
      "  - clipag_vitb32_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.3831709623336792.png\n",
      "  - ... and 545 more files\n",
      "\n",
      "Found 550 PNG files in 02-05-2025_red_20250428-20250430_radio_v2.5-b_accentuation\n",
      "  - radio_v2.5-b_RidgeCV_unit_0_img_0_level_-0.2282366166114813_score_-0.21980516612529755.png\n",
      "  - radio_v2.5-b_RidgeCV_unit_0_img_0_level_-1.0373457669019694_score_-1.0274107456207275.png\n",
      "  - radio_v2.5-b_RidgeCV_unit_0_img_0_level_-1.8464549171924576_score_-1.7680553197860718.png\n",
      "  - radio_v2.5-b_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5726814270019531.png\n",
      "  - radio_v2.5-b_RidgeCV_unit_0_img_0_level_1.389981683969495_score_1.380806565284729.png\n",
      "  - ... and 545 more files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all png files in each subfolder (one level deep)\n",
    "all_png_files = {}\n",
    "for subfolder in target_subfolder:\n",
    "    subfolder_name = os.path.basename(subfolder)\n",
    "    png_files = glob.glob(join(subfolder, \"*.png\"))\n",
    "    # Sort the files to ensure consistent order\n",
    "    png_files.sort()\n",
    "    all_png_files[subfolder_name] = png_files\n",
    "    print(f\"Found {len(png_files)} PNG files in {subfolder_name}\")\n",
    "    # Print first few files as example\n",
    "    for file in png_files[:5]:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    if len(png_files) > 5:\n",
    "        print(f\"  - ... and {len(png_files)-5} more files\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_accentuated_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse accentuated image filenames to extract metadata.\n",
    "    \n",
    "    Example filename format:\n",
    "    radio_v2.5-b_RidgeCV_unit_0_img_0_level_0.580872533679007_score_0.5726814270019531.png\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing extracted metadata (model_name, unit_id, img_id, level, score)\n",
    "    \"\"\"\n",
    "    # Get just the filename without path\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    # Extract components using regex\n",
    "    pattern = r'(.+?)_RidgeCV_unit_(\\d+)_img_(\\d+)_level_([-\\d\\.]+)_score_([-\\d\\.]+)\\.png'\n",
    "    match = re.search(pattern, basename)\n",
    "    \n",
    "    if match:\n",
    "        model_name, unit_id, img_id, level, score = match.groups()\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'unit_id': int(unit_id),\n",
    "            'img_id': int(img_id),\n",
    "            'level': float(level),\n",
    "            'score': float(score),\n",
    "            'filepath': filename\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_accentuated_filenames_to_df(filenames):\n",
    "    \"\"\"\n",
    "    Parse a list of accentuated image filenames and return as a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        filenames (list): List of filenames to parse\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing parsed metadata for all files\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    parsed_data = []\n",
    "    for filename in filenames:\n",
    "        parsed = parse_accentuated_filename(filename)\n",
    "        if parsed:\n",
    "            parsed_data.append(parsed)\n",
    "    \n",
    "    if parsed_data:\n",
    "        return pd.DataFrame(parsed_data)\n",
    "    else:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (5500, 6)\n",
      "\n",
      "DataFrame columns:\n",
      "['model_name', 'unit_id', 'img_id', 'level', 'score', 'filepath']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>img_id</th>\n",
       "      <th>level</th>\n",
       "      <th>score</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.228237</td>\n",
       "      <td>-0.219138</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.037346</td>\n",
       "      <td>-1.027777</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.846455</td>\n",
       "      <td>-1.192481</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.580873</td>\n",
       "      <td>0.572720</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.389982</td>\n",
       "      <td>1.387294</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name  unit_id  img_id     level     score  \\\n",
       "0  siglip2_vitb16        0       0 -0.228237 -0.219138   \n",
       "1  siglip2_vitb16        0       0 -1.037346 -1.027777   \n",
       "2  siglip2_vitb16        0       0 -1.846455 -1.192481   \n",
       "3  siglip2_vitb16        0       0  0.580873  0.572720   \n",
       "4  siglip2_vitb16        0       0  1.389982  1.387294   \n",
       "\n",
       "                                            filepath  \n",
       "0  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...  \n",
       "1  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...  \n",
       "2  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...  \n",
       "3  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...  \n",
       "4  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique models: 10\n",
      "Unique units: 5\n",
      "Unique image IDs: 10\n",
      "Unique levels: 55\n",
      "Unique scores: 5489\n"
     ]
    }
   ],
   "source": [
    "# Test the function on the file list\n",
    "import re\n",
    "import pandas as pd\n",
    "# Parse all files into a DataFrame\n",
    "png_files_list = sum(all_png_files.values(), [])\n",
    "df_accentuated = parse_accentuated_filenames_to_df(png_files_list)\n",
    "# Display info about the DataFrame\n",
    "print(f\"DataFrame shape: {df_accentuated.shape}\")\n",
    "print(\"\\nDataFrame columns:\")\n",
    "print(df_accentuated.columns.tolist())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df_accentuated.head())\n",
    "\n",
    "# Show unique models and units\n",
    "print(f\"\\nUnique models: {df_accentuated['model_name'].nunique()}\")\n",
    "print(f\"Unique units: {df_accentuated['unit_id'].nunique()}\")\n",
    "print(f\"Unique image IDs: {df_accentuated['img_id'].nunique()}\")\n",
    "print(f\"Unique levels: {df_accentuated['level'].nunique()}\")\n",
    "print(f\"Unique scores: {df_accentuated['score'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_toolkit.dataset_utils import ImagePathDataset, DataLoader\n",
    "def get_prediction_responses(pred_fn, transforms_pipeline, image_fps, device=\"cuda\", batch_size=100, num_workers=16):\n",
    "    \"\"\"\n",
    "    Get the prediction responses for a set of images\n",
    "    \"\"\"\n",
    "    dataset = ImagePathDataset(image_fps, transform=transforms_pipeline)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    pred_resp = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch_img = batch[0].to(device)\n",
    "        with th.no_grad():\n",
    "            batch_resp = pred_fn(batch_img).cpu()\n",
    "        pred_resp.append(batch_resp)\n",
    "    return torch.cat(pred_resp, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model AlexNet_training_seed_01, layer .features.MaxPool2d12\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_readout_.features.MaxPool2d12_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_Xtfmer_.features.MaxPool2d12_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    }
   ],
   "source": [
    "config_root = r\"/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs\"\n",
    "subject_id = \"red_20250428-20250430\"\n",
    "config_dir = join(config_root, subject_id)\n",
    "config_files = sorted(glob.glob(join(config_dir, \"*.yaml\")))\n",
    "chan_pattern = \"_Ch19_\"\n",
    "config_pre_chan = [f for f in config_files if chan_pattern in f]\n",
    "config_file = config_pre_chan[0]\n",
    "# for config_file in config_pre_chan:\n",
    "population_predictor, target_unit_predictor, \\\n",
    "    model, transforms_pipeline, _, _, _ \\\n",
    "        = get_predictor_from_config(config_file, device=\"cuda\")\n",
    "\n",
    "accentuated_dataset = ImagePathDataset(df_accentuated[\"filepath\"], transform=transforms_pipeline)\n",
    "accentuated_dataloader = DataLoader(accentuated_dataset, batch_size=100, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb96cefcf7254482a421b63bbeec9f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in:   File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>self._shutdown_workers()\n",
      "\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb51f1fe57c4957b07bc25d001b7da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:03<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "        self._shutdown_workers()if w.is_alive():\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
      "\n",
      "AssertionError  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      ": can only test a child process\n",
      "    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "can only test a child process  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    \n",
      "if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child processException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0><function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "        self._shutdown_workers()self._shutdown_workers()\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError\n",
      ": AssertionErrorcan only test a child process: \n",
      "can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionErrorException ignored in: : <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "can only test a child processTraceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5500, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_img_pop_resp = get_prediction_responses(population_predictor, transforms_pipeline, \n",
    "                                        df_accentuated[\"filepath\"])\n",
    "acc_img_pop_resp.shape # (n_images, n_units)\n",
    "\n",
    "acc_img_chan_resp = get_prediction_responses(target_unit_predictor, transforms_pipeline, \n",
    "                                        df_accentuated[\"filepath\"])\n",
    "acc_img_chan_resp.shape # (n_images, 1) usually only target unit\n",
    "assert torch.allclose(acc_img_pop_resp[:, unit_ids], acc_img_chan_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>img_id</th>\n",
       "      <th>level</th>\n",
       "      <th>score</th>\n",
       "      <th>filepath</th>\n",
       "      <th>pred_resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.228237</td>\n",
       "      <td>-0.219138</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.550313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.037346</td>\n",
       "      <td>-1.027777</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.561994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.846455</td>\n",
       "      <td>-1.192481</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.502087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.580873</td>\n",
       "      <td>0.572720</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.464899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.389982</td>\n",
       "      <td>1.387294</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.389921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name  unit_id  img_id     level     score  \\\n",
       "0  siglip2_vitb16        0       0 -0.228237 -0.219138   \n",
       "1  siglip2_vitb16        0       0 -1.037346 -1.027777   \n",
       "2  siglip2_vitb16        0       0 -1.846455 -1.192481   \n",
       "3  siglip2_vitb16        0       0  0.580873  0.572720   \n",
       "4  siglip2_vitb16        0       0  1.389982  1.387294   \n",
       "\n",
       "                                            filepath  pred_resp  \n",
       "0  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   0.550313  \n",
       "1  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   0.561994  \n",
       "2  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   0.502087  \n",
       "3  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   0.464899  \n",
       "4  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   0.389921  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_pred_resp = df_accentuated.copy()\n",
    "# df_with_pred_resp[\"pop_resp\"] = acc_img_pop_resp.cpu().numpy()\n",
    "df_with_pred_resp[\"pred_resp\"] = acc_img_chan_resp.cpu().numpy()\n",
    "df_with_pred_resp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df_with_pred_resp.query(\"unit_id == 19\"), x=\"score\", y=\"pred_resp\", hue=\"model_name\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass produce cross prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model AlexNet_training_seed_01, layer .features.MaxPool2d12\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch00_readout_.features.MaxPool2d12_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch00_Xtfmer_.features.MaxPool2d12_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cbe3d55e35426c8410dc4f54ed9cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model AlexNet_training_seed_01, layer .features.MaxPool2d12\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch15_readout_.features.MaxPool2d12_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch15_Xtfmer_.features.MaxPool2d12_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122126d185c54f4cb1efd1b0b6f878bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model AlexNet_training_seed_01, layer .features.MaxPool2d12\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_readout_.features.MaxPool2d12_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch19_Xtfmer_.features.MaxPool2d12_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bd82c6e03f423e872452fb29182653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model AlexNet_training_seed_01, layer .features.MaxPool2d5\n",
      "Target unit ids: [2]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch02_readout_.features.MaxPool2d5_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch02_Xtfmer_.features.MaxPool2d5_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901c95f8901d4ea185236965e41665ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "\n",
      "Traceback (most recent call last):\n",
      "      File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "self._shutdown_workers()    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "\n",
      "Exception ignored in:   File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>if w.is_alive():\n",
      "\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "self._shutdown_workers()AssertionError\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      ": AssertionError    can only test a child process: if w.is_alive():\n",
      "can only test a child process\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: \n",
      "can only test a child processTraceback (most recent call last):\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14dcf8f6bac0>\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "\n",
      "    Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "if w.is_alive():    \n",
      "self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
      "can only test a child process\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model AlexNet_training_seed_01, layer .features.ReLU11\n",
      "Target unit ids: [9]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch09_readout_.features.ReLU11_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_AlexNet_training_seed_01_Ch09_Xtfmer_.features.ReLU11_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470d0ad2ba4c400ebba8532d6ae23e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model clipag_vitb32, layer .transformer.resblocks.ResidualAttentionBlock8\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch00_readout_.transformer.resblocks.ResidualAttentionBlock8_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch00_Xtfmer_.transformer.resblocks.ResidualAttentionBlock8_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8ade804d4e4d0c8e291ca1fb612d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model clipag_vitb32, layer .transformer.resblocks.ResidualAttentionBlock6\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch15_readout_.transformer.resblocks.ResidualAttentionBlock6_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch15_Xtfmer_.transformer.resblocks.ResidualAttentionBlock6_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fa16b6683249729e11cd5d9d2bc86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model clipag_vitb32, layer .transformer.resblocks.ResidualAttentionBlock9\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch19_readout_.transformer.resblocks.ResidualAttentionBlock9_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch19_Xtfmer_.transformer.resblocks.ResidualAttentionBlock9_pca750_RidgeCV_JITscript.pt\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516694ef34644d10a4aa504af71cfce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model clipag_vitb32, layer .transformer.resblocks.ResidualAttentionBlock8\n",
      "Target unit ids: [2]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch02_readout_.transformer.resblocks.ResidualAttentionBlock8_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch02_Xtfmer_.transformer.resblocks.ResidualAttentionBlock8_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9368ad5f313b4fe0b50fea60258a6721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model clipag_vitb32, layer .transformer.resblocks.ResidualAttentionBlock8\n",
      "Target unit ids: [9]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch09_readout_.transformer.resblocks.ResidualAttentionBlock8_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_clipag_vitb32_Ch09_Xtfmer_.transformer.resblocks.ResidualAttentionBlock8_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8682017de2df4eebb2feba26f530533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model dinov2_vitb14_reg, layer .blocks.NestedTensorBlock8\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch00_readout_.blocks.NestedTensorBlock8_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch00_Xtfmer_.blocks.NestedTensorBlock8_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e095a3beeb584c718f2503518b00026d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model dinov2_vitb14_reg, layer .blocks.NestedTensorBlock8\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch15_readout_.blocks.NestedTensorBlock8_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch15_Xtfmer_.blocks.NestedTensorBlock8_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45dc63812ad483584da91b4ce874550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model dinov2_vitb14_reg, layer .blocks.NestedTensorBlock11\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch19_readout_.blocks.NestedTensorBlock11_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch19_Xtfmer_.blocks.NestedTensorBlock11_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f6f914e78f4b748c7c49ebdebd1de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model dinov2_vitb14_reg, layer .blocks.NestedTensorBlock10\n",
      "Target unit ids: [2]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch02_readout_.blocks.NestedTensorBlock10_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch02_Xtfmer_.blocks.NestedTensorBlock10_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e33f10a162b4effad52bef5f7c0a7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model dinov2_vitb14_reg, layer .blocks.NestedTensorBlock11\n",
      "Target unit ids: [9]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch09_readout_.blocks.NestedTensorBlock11_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_dinov2_vitb14_reg_Ch09_Xtfmer_.blocks.NestedTensorBlock11_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dinov2_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb1a559aca541579ed66f8ed184667c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model radio_v2.5-b, layer .model.blocks.Block10\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch00_readout_.model.blocks.Block10_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch00_Xtfmer_.model.blocks.Block10_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/NVlabs_RADIO_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97c0a85a66d4b48bb4fcb53cbdfb573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model radio_v2.5-b, layer .model.blocks.Block10\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch15_readout_.model.blocks.Block10_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch15_Xtfmer_.model.blocks.Block10_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/NVlabs_RADIO_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1241404c612d4d4c8dad376df67f519a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model radio_v2.5-b, layer .model.blocks.Block10\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch19_readout_.model.blocks.Block10_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch19_Xtfmer_.model.blocks.Block10_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/NVlabs_RADIO_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b6a9ced8ec40b483db65a3513c1a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model radio_v2.5-b, layer .model.blocks.Block11\n",
      "Target unit ids: [2]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch02_readout_.model.blocks.Block11_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch02_Xtfmer_.model.blocks.Block11_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/NVlabs_RADIO_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c478d33c3b149058a25d4886669cccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model radio_v2.5-b, layer .model.blocks.Block10\n",
      "Target unit ids: [9]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch09_readout_.model.blocks.Block10_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_radio_v2.5-b_Ch09_Xtfmer_.model.blocks.Block10_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/NVlabs_RADIO_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94d5c5a61364f2a828ad1c9a2e0be8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model regnety_640, layer .s3.Bottleneckb9\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch00_readout_.s3.Bottleneckb9_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch00_Xtfmer_.s3.Bottleneckb9_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1db2017c4e4411585c457a0555471d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model regnety_640, layer .s3.Bottleneckb9\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch15_readout_.s3.Bottleneckb9_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch15_Xtfmer_.s3.Bottleneckb9_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f51c06c157406ab86cd3cea1523fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model regnety_640, layer .s4.Bottleneckb1\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch19_readout_.s4.Bottleneckb1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch19_Xtfmer_.s4.Bottleneckb1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946e7651a2614939be9890fd92b003ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model regnety_640, layer .s4.Bottleneckb1\n",
      "Target unit ids: [2]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch02_readout_.s4.Bottleneckb1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch02_Xtfmer_.s4.Bottleneckb1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bea4a25c874ba286dcd4115288df4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model regnety_640, layer .s4.Bottleneckb1\n",
      "Target unit ids: [9]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch09_readout_.s4.Bottleneckb1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_regnety_640_Ch09_Xtfmer_.s4.Bottleneckb1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5dcb1f477b4eceb063d44b27e039ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50, layer .layer4.Bottleneck0\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch00_readout_.layer4.Bottleneck0_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch00_Xtfmer_.layer4.Bottleneck0_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a48b3796e04251bda301cdac94c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50, layer .layer3.Bottleneck5\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch15_readout_.layer3.Bottleneck5_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch15_Xtfmer_.layer3.Bottleneck5_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0fbcadcd5944be8f8d3f2260c75ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50, layer .layer4.Bottleneck0\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch19_readout_.layer4.Bottleneck0_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch19_Xtfmer_.layer4.Bottleneck0_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3544f7e6ba774855a677f3961074157d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50, layer .layer4.Bottleneck1\n",
      "Target unit ids: [2]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch02_readout_.layer4.Bottleneck1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch02_Xtfmer_.layer4.Bottleneck1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3b8c9e1b87480b8943ae5496c29664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50, layer .layer4.Bottleneck0\n",
      "Target unit ids: [9]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch09_readout_.layer4.Bottleneck0_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_Ch09_Xtfmer_.layer4.Bottleneck0_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1626756310264139a111731314d636f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_clip, layer .layer4.Bottleneck0\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch00_readout_.layer4.Bottleneck0_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch00_Xtfmer_.layer4.Bottleneck0_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3aa9acf3c647ff9d804a521942ed0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_clip, layer .layer3.Bottleneck5\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch15_readout_.layer3.Bottleneck5_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch15_Xtfmer_.layer3.Bottleneck5_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af8a88af20e496b8a8b2117cfa098bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_clip, layer .layer4.Bottleneck1\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch19_readout_.layer4.Bottleneck1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch19_Xtfmer_.layer4.Bottleneck1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db370276e964817906712c578ba8d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_clip, layer .layer4.Bottleneck0\n",
      "Target unit ids: [2]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch02_readout_.layer4.Bottleneck0_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch02_Xtfmer_.layer4.Bottleneck0_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db60cf013c4894bb281e14dd753e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_clip, layer .layer4.Bottleneck1\n",
      "Target unit ids: [9]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch09_readout_.layer4.Bottleneck1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_clip_Ch09_Xtfmer_.layer4.Bottleneck1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b2983e618c453b965eb6287ef0bac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_dino, layer .layer4.Bottleneck0\n",
      "Target unit ids: [0]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch00_readout_.layer4.Bottleneck0_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch00_Xtfmer_.layer4.Bottleneck0_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dino_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa47a318c4f04d72bb85b31c6b509779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_dino, layer .layer4.Bottleneck2\n",
      "Target unit ids: [15]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch15_readout_.layer4.Bottleneck2_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch15_Xtfmer_.layer4.Bottleneck2_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dino_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b99ecb0a474a48836e5df49d6a5a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and set up feature extraction: model resnet50_dino, layer .layer4.Bottleneck1\n",
      "Target unit ids: [19]\n",
      "Loading readout layer and PCA transform: readout /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch19_readout_.layer4.Bottleneck1_pca750_RidgeCV.pth, xtransform             /n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/Encoding_model_outputs/red_20250428-20250430/red_20250428-20250430_resnet50_dino_Ch19_Xtfmer_.layer4.Bottleneck1_pca750_RidgeCV_JITscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/torch_cache/hub/facebookresearch_dino_main\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/serialization.py:1006: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38badd83261848f3abb064268f4714c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m model_name \u001b[38;5;241m=\u001b[39m acc_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m population_predictor, target_unit_predictor, \\\n\u001b[1;32m     17\u001b[0m     model, transforms_pipeline, _, _, _ \\\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m=\u001b[39m get_predictor_from_config(config_file, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m acc_img_chan_resp \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_unit_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdf_accentuated\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilepath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m pred_resp_dict[(model_name, unit_ids)] \u001b[38;5;241m=\u001b[39m acc_img_chan_resp\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     22\u001b[0m df_acc_w_pred_resp[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_resp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_unit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m acc_img_chan_resp\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m, in \u001b[0;36mget_prediction_responses\u001b[0;34m(pred_fn, transforms_pipeline, image_fps, device, batch_size, num_workers)\u001b[0m\n\u001b[1;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n\u001b[1;32m      8\u001b[0m pred_resp \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     10\u001b[0m     batch_img \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_root = r\"/n/holylabs/LABS/alvarez_lab/Everyone/Accentuate_VVS/accentuation_configs\"\n",
    "subject_id = \"red_20250428-20250430\"\n",
    "config_dir = join(config_root, subject_id)\n",
    "config_files = sorted(glob.glob(join(config_dir, \"*.yaml\")))\n",
    "# chan_pattern = \"_Ch19_\"\n",
    "# config_pre_chan = [f for f in config_files if chan_pattern in f]\n",
    "# config_file = config_pre_chan[0]\n",
    "pred_resp_dict = {}\n",
    "df_acc_w_pred_resp = df_accentuated.copy()\n",
    "for config_file in config_files:\n",
    "    acc_config = yaml.safe_load(open(config_file))\n",
    "    unit_ids = acc_config['unit_ids']\n",
    "    assert len(unit_ids) == 1, \"Only one unit is supported for now\"\n",
    "    unit_ids = unit_ids[0]\n",
    "    model_name = acc_config['model_name']\n",
    "    population_predictor, target_unit_predictor, \\\n",
    "        model, transforms_pipeline, _, _, _ \\\n",
    "            = get_predictor_from_config(config_file, device=\"cuda\")\n",
    "    acc_img_chan_resp = get_prediction_responses(target_unit_predictor, transforms_pipeline, \n",
    "                                            df_accentuated[\"filepath\"])\n",
    "    pred_resp_dict[(model_name, unit_ids)] = acc_img_chan_resp.cpu().numpy()\n",
    "    df_acc_w_pred_resp[f\"pred_resp_{model_name}_unit_{unit_ids}\"] = acc_img_chan_resp.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_dir = f\"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Encoding_models/{subject_id}/posthoc_model_predict\"\n",
    "os.makedirs(posthoc_dir, exist_ok=True)\n",
    "df_accentuated.to_pickle(join(posthoc_dir, f\"accentuated_stim_info_{subject_id}.pkl\"))\n",
    "df_accentuated.to_csv(join(posthoc_dir, f\"accentuated_stim_info_{subject_id}.csv\"))\n",
    "df_acc_w_pred_resp.to_pickle(join(posthoc_dir, f\"accentuated_stim_info_w_pred_resp_{subject_id}.pkl\"))\n",
    "pkl.dump(pred_resp_dict, open(join(posthoc_dir, f\"pred_resp_dict_{subject_id}.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_dir = f\"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Encoding_models/{subject_id}/posthoc_model_predict\"\n",
    "os.makedirs(posthoc_dir, exist_ok=True)\n",
    "df_acc_w_pred_resp = pd.read_pickle(join(posthoc_dir, f\"accentuated_stim_info_w_pred_resp_{subject_id}.pkl\"))\n",
    "pred_resp_dict = pkl.load(open(join(posthoc_dir, f\"pred_resp_dict_{subject_id}.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>img_id</th>\n",
       "      <th>level</th>\n",
       "      <th>score</th>\n",
       "      <th>filepath</th>\n",
       "      <th>pred_resp_AlexNet_training_seed_01_unit_0</th>\n",
       "      <th>pred_resp_AlexNet_training_seed_01_unit_15</th>\n",
       "      <th>pred_resp_AlexNet_training_seed_01_unit_19</th>\n",
       "      <th>pred_resp_AlexNet_training_seed_01_unit_2</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_resp_resnet50_robust_unit_0</th>\n",
       "      <th>pred_resp_resnet50_robust_unit_15</th>\n",
       "      <th>pred_resp_resnet50_robust_unit_19</th>\n",
       "      <th>pred_resp_resnet50_robust_unit_2</th>\n",
       "      <th>pred_resp_resnet50_robust_unit_9</th>\n",
       "      <th>pred_resp_siglip2_vitb16_unit_0</th>\n",
       "      <th>pred_resp_siglip2_vitb16_unit_15</th>\n",
       "      <th>pred_resp_siglip2_vitb16_unit_19</th>\n",
       "      <th>pred_resp_siglip2_vitb16_unit_2</th>\n",
       "      <th>pred_resp_siglip2_vitb16_unit_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlexNet_training_seed_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.228237</td>\n",
       "      <td>-0.221289</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>-0.215352</td>\n",
       "      <td>-0.048412</td>\n",
       "      <td>-0.032983</td>\n",
       "      <td>0.312984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678680</td>\n",
       "      <td>0.822875</td>\n",
       "      <td>1.137569</td>\n",
       "      <td>1.059690</td>\n",
       "      <td>1.138820</td>\n",
       "      <td>0.177207</td>\n",
       "      <td>0.327468</td>\n",
       "      <td>0.359148</td>\n",
       "      <td>0.677599</td>\n",
       "      <td>0.348708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlexNet_training_seed_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.037346</td>\n",
       "      <td>-1.027856</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>-1.030597</td>\n",
       "      <td>-0.547346</td>\n",
       "      <td>-0.718770</td>\n",
       "      <td>0.264506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666396</td>\n",
       "      <td>0.790431</td>\n",
       "      <td>1.157040</td>\n",
       "      <td>1.017175</td>\n",
       "      <td>1.147675</td>\n",
       "      <td>0.297980</td>\n",
       "      <td>0.394564</td>\n",
       "      <td>0.400089</td>\n",
       "      <td>0.560250</td>\n",
       "      <td>0.432954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlexNet_training_seed_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.846455</td>\n",
       "      <td>-1.838532</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>-1.840062</td>\n",
       "      <td>-1.077613</td>\n",
       "      <td>-1.406070</td>\n",
       "      <td>0.208471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626149</td>\n",
       "      <td>0.732566</td>\n",
       "      <td>1.169050</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>1.145592</td>\n",
       "      <td>0.301777</td>\n",
       "      <td>0.365760</td>\n",
       "      <td>0.453462</td>\n",
       "      <td>0.503688</td>\n",
       "      <td>0.471211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlexNet_training_seed_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.580873</td>\n",
       "      <td>0.572135</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.580558</td>\n",
       "      <td>0.408522</td>\n",
       "      <td>0.621783</td>\n",
       "      <td>0.270804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664914</td>\n",
       "      <td>0.816001</td>\n",
       "      <td>1.122917</td>\n",
       "      <td>1.029036</td>\n",
       "      <td>1.127711</td>\n",
       "      <td>0.121254</td>\n",
       "      <td>0.343337</td>\n",
       "      <td>0.375945</td>\n",
       "      <td>0.712595</td>\n",
       "      <td>0.344175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlexNet_training_seed_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.389982</td>\n",
       "      <td>1.380919</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>1.389751</td>\n",
       "      <td>0.828179</td>\n",
       "      <td>1.236538</td>\n",
       "      <td>0.221984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650314</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>1.163512</td>\n",
       "      <td>0.985825</td>\n",
       "      <td>1.166690</td>\n",
       "      <td>0.219089</td>\n",
       "      <td>0.379959</td>\n",
       "      <td>0.363606</td>\n",
       "      <td>0.496675</td>\n",
       "      <td>0.362347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2.701443</td>\n",
       "      <td>2.691693</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.675457</td>\n",
       "      <td>0.179394</td>\n",
       "      <td>0.570379</td>\n",
       "      <td>-0.125893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408323</td>\n",
       "      <td>0.443555</td>\n",
       "      <td>0.057397</td>\n",
       "      <td>0.191934</td>\n",
       "      <td>0.037918</td>\n",
       "      <td>2.072134</td>\n",
       "      <td>1.186616</td>\n",
       "      <td>2.423980</td>\n",
       "      <td>0.319658</td>\n",
       "      <td>2.686192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3.442825</td>\n",
       "      <td>3.434636</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.496783</td>\n",
       "      <td>0.132663</td>\n",
       "      <td>0.397479</td>\n",
       "      <td>-0.125035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405369</td>\n",
       "      <td>0.433399</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.194160</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>2.505370</td>\n",
       "      <td>1.492182</td>\n",
       "      <td>3.184868</td>\n",
       "      <td>0.360922</td>\n",
       "      <td>3.430079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4.184206</td>\n",
       "      <td>4.180340</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.514549</td>\n",
       "      <td>0.225759</td>\n",
       "      <td>0.436111</td>\n",
       "      <td>-0.121330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413317</td>\n",
       "      <td>0.419002</td>\n",
       "      <td>0.027742</td>\n",
       "      <td>0.201799</td>\n",
       "      <td>0.020518</td>\n",
       "      <td>2.701764</td>\n",
       "      <td>1.564144</td>\n",
       "      <td>3.877917</td>\n",
       "      <td>0.409598</td>\n",
       "      <td>4.169995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4.925587</td>\n",
       "      <td>4.921776</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.305701</td>\n",
       "      <td>-0.034255</td>\n",
       "      <td>0.171149</td>\n",
       "      <td>-0.112103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370868</td>\n",
       "      <td>0.370514</td>\n",
       "      <td>-0.012873</td>\n",
       "      <td>0.205893</td>\n",
       "      <td>-0.012511</td>\n",
       "      <td>3.149135</td>\n",
       "      <td>1.826766</td>\n",
       "      <td>4.706793</td>\n",
       "      <td>0.256107</td>\n",
       "      <td>4.929672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>siglip2_vitb16</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5.666968</td>\n",
       "      <td>5.657128</td>\n",
       "      <td>/n/holylabs/LABS/alvarez_lab/Everyone/Accentua...</td>\n",
       "      <td>0.195528</td>\n",
       "      <td>0.101603</td>\n",
       "      <td>0.184035</td>\n",
       "      <td>-0.134632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358734</td>\n",
       "      <td>0.345463</td>\n",
       "      <td>-0.033911</td>\n",
       "      <td>0.190913</td>\n",
       "      <td>-0.031214</td>\n",
       "      <td>3.402702</td>\n",
       "      <td>2.149546</td>\n",
       "      <td>5.396996</td>\n",
       "      <td>0.236318</td>\n",
       "      <td>5.663766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5500 rows  56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name  unit_id  img_id     level     score  \\\n",
       "0     AlexNet_training_seed_01        0       0 -0.228237 -0.221289   \n",
       "1     AlexNet_training_seed_01        0       0 -1.037346 -1.027856   \n",
       "2     AlexNet_training_seed_01        0       0 -1.846455 -1.838532   \n",
       "3     AlexNet_training_seed_01        0       0  0.580873  0.572135   \n",
       "4     AlexNet_training_seed_01        0       0  1.389982  1.380919   \n",
       "...                        ...      ...     ...       ...       ...   \n",
       "5495            siglip2_vitb16        9       9  2.701443  2.691693   \n",
       "5496            siglip2_vitb16        9       9  3.442825  3.434636   \n",
       "5497            siglip2_vitb16        9       9  4.184206  4.180340   \n",
       "5498            siglip2_vitb16        9       9  4.925587  4.921776   \n",
       "5499            siglip2_vitb16        9       9  5.666968  5.657128   \n",
       "\n",
       "                                               filepath  \\\n",
       "0     /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "1     /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "2     /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "3     /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "4     /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "...                                                 ...   \n",
       "5495  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "5496  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "5497  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "5498  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "5499  /n/holylabs/LABS/alvarez_lab/Everyone/Accentua...   \n",
       "\n",
       "      pred_resp_AlexNet_training_seed_01_unit_0  \\\n",
       "0                                     -0.215352   \n",
       "1                                     -1.030597   \n",
       "2                                     -1.840062   \n",
       "3                                      0.580558   \n",
       "4                                      1.389751   \n",
       "...                                         ...   \n",
       "5495                                   0.675457   \n",
       "5496                                   0.496783   \n",
       "5497                                   0.514549   \n",
       "5498                                   0.305701   \n",
       "5499                                   0.195528   \n",
       "\n",
       "      pred_resp_AlexNet_training_seed_01_unit_15  \\\n",
       "0                                      -0.048412   \n",
       "1                                      -0.547346   \n",
       "2                                      -1.077613   \n",
       "3                                       0.408522   \n",
       "4                                       0.828179   \n",
       "...                                          ...   \n",
       "5495                                    0.179394   \n",
       "5496                                    0.132663   \n",
       "5497                                    0.225759   \n",
       "5498                                   -0.034255   \n",
       "5499                                    0.101603   \n",
       "\n",
       "      pred_resp_AlexNet_training_seed_01_unit_19  \\\n",
       "0                                      -0.032983   \n",
       "1                                      -0.718770   \n",
       "2                                      -1.406070   \n",
       "3                                       0.621783   \n",
       "4                                       1.236538   \n",
       "...                                          ...   \n",
       "5495                                    0.570379   \n",
       "5496                                    0.397479   \n",
       "5497                                    0.436111   \n",
       "5498                                    0.171149   \n",
       "5499                                    0.184035   \n",
       "\n",
       "      pred_resp_AlexNet_training_seed_01_unit_2  ...  \\\n",
       "0                                      0.312984  ...   \n",
       "1                                      0.264506  ...   \n",
       "2                                      0.208471  ...   \n",
       "3                                      0.270804  ...   \n",
       "4                                      0.221984  ...   \n",
       "...                                         ...  ...   \n",
       "5495                                  -0.125893  ...   \n",
       "5496                                  -0.125035  ...   \n",
       "5497                                  -0.121330  ...   \n",
       "5498                                  -0.112103  ...   \n",
       "5499                                  -0.134632  ...   \n",
       "\n",
       "      pred_resp_resnet50_robust_unit_0  pred_resp_resnet50_robust_unit_15  \\\n",
       "0                             0.678680                           0.822875   \n",
       "1                             0.666396                           0.790431   \n",
       "2                             0.626149                           0.732566   \n",
       "3                             0.664914                           0.816001   \n",
       "4                             0.650314                           0.816000   \n",
       "...                                ...                                ...   \n",
       "5495                          0.408323                           0.443555   \n",
       "5496                          0.405369                           0.433399   \n",
       "5497                          0.413317                           0.419002   \n",
       "5498                          0.370868                           0.370514   \n",
       "5499                          0.358734                           0.345463   \n",
       "\n",
       "      pred_resp_resnet50_robust_unit_19  pred_resp_resnet50_robust_unit_2  \\\n",
       "0                              1.137569                          1.059690   \n",
       "1                              1.157040                          1.017175   \n",
       "2                              1.169050                          0.938542   \n",
       "3                              1.122917                          1.029036   \n",
       "4                              1.163512                          0.985825   \n",
       "...                                 ...                               ...   \n",
       "5495                           0.057397                          0.191934   \n",
       "5496                           0.042641                          0.194160   \n",
       "5497                           0.027742                          0.201799   \n",
       "5498                          -0.012873                          0.205893   \n",
       "5499                          -0.033911                          0.190913   \n",
       "\n",
       "      pred_resp_resnet50_robust_unit_9  pred_resp_siglip2_vitb16_unit_0  \\\n",
       "0                             1.138820                         0.177207   \n",
       "1                             1.147675                         0.297980   \n",
       "2                             1.145592                         0.301777   \n",
       "3                             1.127711                         0.121254   \n",
       "4                             1.166690                         0.219089   \n",
       "...                                ...                              ...   \n",
       "5495                          0.037918                         2.072134   \n",
       "5496                          0.029552                         2.505370   \n",
       "5497                          0.020518                         2.701764   \n",
       "5498                         -0.012511                         3.149135   \n",
       "5499                         -0.031214                         3.402702   \n",
       "\n",
       "      pred_resp_siglip2_vitb16_unit_15  pred_resp_siglip2_vitb16_unit_19  \\\n",
       "0                             0.327468                          0.359148   \n",
       "1                             0.394564                          0.400089   \n",
       "2                             0.365760                          0.453462   \n",
       "3                             0.343337                          0.375945   \n",
       "4                             0.379959                          0.363606   \n",
       "...                                ...                               ...   \n",
       "5495                          1.186616                          2.423980   \n",
       "5496                          1.492182                          3.184868   \n",
       "5497                          1.564144                          3.877917   \n",
       "5498                          1.826766                          4.706793   \n",
       "5499                          2.149546                          5.396996   \n",
       "\n",
       "      pred_resp_siglip2_vitb16_unit_2  pred_resp_siglip2_vitb16_unit_9  \n",
       "0                            0.677599                         0.348708  \n",
       "1                            0.560250                         0.432954  \n",
       "2                            0.503688                         0.471211  \n",
       "3                            0.712595                         0.344175  \n",
       "4                            0.496675                         0.362347  \n",
       "...                               ...                              ...  \n",
       "5495                         0.319658                         2.686192  \n",
       "5496                         0.360922                         3.430079  \n",
       "5497                         0.409598                         4.169995  \n",
       "5498                         0.256107                         4.929672  \n",
       "5499                         0.236318                         5.663766  \n",
       "\n",
       "[5500 rows x 56 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc_w_pred_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"vvs-accentuate-day1_normalize_red_20241212-20241220.h5\" # day 1 neural data\n",
    "# \"vvs_accentuate_day3_normalize_red_20250123-20250126.hdf5\" # day 3 neural data \n",
    "# \"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Stimuli/results_12-01-2025\" # generated accentuated stimuli  \n",
    "\n",
    "# stimuli_root = \"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Stimuli\"\n",
    "# ephys_root = \"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Ephys_Data\"\n",
    "\n",
    "# datapath_day1 = join(ephys_root, \"vvs-accentuate-day1_normalize_red_20241212-20241220.h5\")\n",
    "# datapath_day3 = join(ephys_root, \"vvs_accentuate_day3_normalize_red_20250123-20250126.hdf5\")\n",
    "\n",
    "# imgdir_shared = join(stimuli_root, \"shared1000\")\n",
    "# imgdir_day1 = join(stimuli_root, \"stimuli_pilot_20241119/results\")\n",
    "# imgdir_day3 = join(stimuli_root, \"results_12-01-2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_fulldict_day1 = load_from_hdf5(datapath_day1,)\n",
    "# # data_fulldict_day3 = load_from_hdf5(datapath_day3,)\n",
    "# subject_id1 = \"red_20241212-20241220\"\n",
    "# subject_id3 = \"red_20250123-20250126\"\n",
    "# data_dict_day1 = load_neural_data(datapath_day1, subject_id1, None)\n",
    "# data_dict_day3 = load_neural_data(datapath_day3, subject_id3, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All stimulus files were found\n",
      "All stimulus files were found\n"
     ]
    }
   ],
   "source": [
    "# data_dict_day1[\"image_fps\"] = parse_image_fullpaths(data_dict_day1[\"stimulus_names\"], [imgdir_shared, imgdir_day1])\n",
    "# data_dict_day3[\"image_fps\"] = parse_image_fullpaths(data_dict_day3[\"stimulus_names\"], [imgdir_shared, imgdir_day3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_root = r\"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Encoding_model_outputs\"\n",
    "encoding_dir = join(encoding_root, \"red_20241212-20241220\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"red_20241212-20241220\"\n",
    "model_name = \"resnet50\"\n",
    "layer_name = \".layer4.Bottleneck1\"\n",
    "dimred_str = \"pca1000\"\n",
    "regressor = \"MultiTaskLassoCV\"\n",
    "meta_path = f\"{subject_id}_{model_name}_meta_{layer_name}_{dimred_str}_{regressor}.pkl\"\n",
    "readout_path = f\"{subject_id}_{model_name}_readout_{layer_name}_{dimred_str}_{regressor}.pth\"\n",
    "Xtransform_path = f\"{subject_id}_{model_name}_Xtfmer_{layer_name}_{dimred_str}_{regressor}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(objective_fn):\n",
    "    \"\"\"Check if gradients can flow through the objective function.\"\"\"\n",
    "    img_opt = th.randn(1, 3, 224, 224).cuda()\n",
    "    img_opt.requires_grad_(True)\n",
    "    resp = objective_fn(img_opt)\n",
    "    resp.mean().backward()\n",
    "    print(resp.shape)\n",
    "    assert img_opt.grad is not None\n",
    "\n",
    "\n",
    "def get_population_predictor_compact(subject_id=\"red_20241212-20241220\",\n",
    "                            model_name=\"resnet50\",\n",
    "                            layer_name=\".layer4.Bottleneck1\",\n",
    "                            dimred_str=\"pca1000\",\n",
    "                            regressor=\"MultiTaskLassoCV\", device=\"cuda\"):\n",
    "    \"\"\"Create a function that predicts neural population responses for images.\n",
    "    \n",
    "    Args:\n",
    "        subject_id (str): ID of the subject\n",
    "        modelname (str): Name of the model to use (e.g. \"resnet50_robust\") \n",
    "        layer_name (str): Name of layer to extract features from\n",
    "        device (str): Device to run model on (\"cuda\" or \"cpu\")\n",
    "        \n",
    "    Returns:\n",
    "        function: A function that takes images as input and returns predicted population responses\n",
    "    \"\"\"\n",
    "    # Construct paths\n",
    "    \n",
    "    model_root = f\"/n/holylabs/LABS/alvarez_lab/Lab/VVS_Accentuation/Encoding_model_outputs/red_20241212-20241220\"\n",
    "    meta_path = join(model_root, f\"{subject_id}_{model_name}_meta_{layer_name}_{dimred_str}_{regressor}.pkl\")\n",
    "    readout_path = join(model_root, f\"{subject_id}_{model_name}_readout_{layer_name}_{dimred_str}_{regressor}.pth\")\n",
    "    Xtransform_path = join(model_root, f\"{subject_id}_{model_name}_Xtfmer_{layer_name}_{dimred_str}_{regressor}.pkl\")\n",
    "    key = (f'{layer_name}_{dimred_str}', regressor)\n",
    "    # readout_path = join(base_path, f\"{subject_id}_{modelname}_sweep_regressors_readout_RidgeCV_{key[0]}.pth\")\n",
    "    # Xtransform_path = join(base_path, f\"{subject_id}_{modelname}_sweep_regressors_Xtfmer_RidgeCV_{key[0]}.pkl\")\n",
    "    \n",
    "    # Load model and set up feature extraction\n",
    "    model, transforms_pipeline = load_model_transform(model_name, device=device)\n",
    "    model = model.eval().to(device)\n",
    "    model.requires_grad_(False)\n",
    "    fetcher = featureFetcher(model, input_size=(3, 224, 224), print_module=False)\n",
    "    fetcher.record(layer_name, ingraph=True, store_device=device)\n",
    "\n",
    "    # Load readout layer\n",
    "    state_dict = th.load(readout_path)\n",
    "    readout = nn.Linear(state_dict['weight'].shape[1], state_dict['weight'].shape[0], bias=True).to(device)\n",
    "    readout.load_state_dict(state_dict)\n",
    "    \n",
    "    # Load PCA transform\n",
    "    pca = pkl.load(open(Xtransform_path, \"rb\"))\n",
    "    Xtransform = PCA_torch(pca, device=device)\n",
    "\n",
    "    def predict_population_response(images):\n",
    "        \"\"\"Predict neural population responses for input images.\n",
    "        \n",
    "        Args:\n",
    "            images (torch.Tensor): Input images of shape (batch_size, 3, 224, 224)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Predicted neural responses\n",
    "        \"\"\"\n",
    "        model(images)\n",
    "        feat_tsr = fetcher[layer_name]\n",
    "        feat_vec = Xtransform(feat_tsr)\n",
    "        return readout(feat_vec)\n",
    "\n",
    "    check_gradient(predict_population_response)\n",
    "    print(\"Gradient check passed!\")\n",
    "    return predict_population_response, model, transforms_pipeline, fetcher, Xtransform, readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n",
      "torch.Size([1, 64])\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "layer_name = \".layer4.Bottleneck1\"\n",
    "dimred_str = \"pca1000\"\n",
    "pred_fn_RN50_Lasso, model_RNLasso, transforms_pipeline, _, _, _ = get_population_predictor(model_name=\"resnet50\", regressor=\"MultiTaskLassoCV\", )\n",
    "pred_fn_RN50_Ridge, model_RNRidge, transforms_pipeline, _, _, _ = get_population_predictor(model_name=\"resnet50\", regressor=\"RidgeCV\", )\n",
    "pred_fn_RN50robust_Lasso, model_RNrbstLasso, transforms_pipeline_rbst, _, _, _ = get_population_predictor(model_name=\"resnet50_robust\", regressor=\"MultiTaskLassoCV\", )\n",
    "pred_fn_RN50robust_Ridge, model_RNrbstRidge, transforms_pipeline_rbst, _, _, _ = get_population_predictor(model_name=\"resnet50_robust\", regressor=\"RidgeCV\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_day1 = ImagePathDataset(data_dict_day1[\"image_fps\"])\n",
    "dataloader_day1 = DataLoader(dataset_day1, batch_size=100, shuffle=False, num_workers=10)\n",
    "dataset_day3 = ImagePathDataset(data_dict_day3[\"image_fps\"])\n",
    "dataloader_day3 = DataLoader(dataset_day3, batch_size=100, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7a181948a24bc7a87296dcc39c30ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "\n",
      "Exception ignored in: Traceback (most recent call last):\n",
      "    <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "self._shutdown_workers()\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "      File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "self._shutdown_workers()    \n",
      "    if w.is_alive():  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "\n",
      "self._shutdown_workers()  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    \n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "if w.is_alive():    \n",
      "      File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "if w.is_alive():AssertionError\n",
      "Exception ignored in:       File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      ": <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError\n",
      ": AssertionErrorTraceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      ": can only test a child process    can only test a child processself._shutdown_workers()\n",
      "\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>Exception ignored in: \n",
      "Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()    self._shutdown_workers()\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "\n",
      "      File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionErrorAssertionError: can only test a child process: can only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6758cd4ffa444b0f827a2c17e8379020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908296c3c8f9442aabb53cab17845d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566e694b416c4480b51a3406f14a472a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0><function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "        self._shutdown_workers()self._shutdown_workers()\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        if w.is_alive():if w.is_alive():\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionErrorAssertionError: : can only test a child processcan only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>self._shutdown_workers()\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "      File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "if w.is_alive():    \n",
      "self._shutdown_workers()  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "      File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "if w.is_alive():AssertionError\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      ":     can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in:   File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x14994a833ac0>    \n",
      "self._shutdown_workers()Traceback (most recent call last):\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        self._shutdown_workers()if w.is_alive():\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
      "\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
      "\n",
      "AssertionError: can only test a child process\n"
     ]
    }
   ],
   "source": [
    "population_pred_day3_RN50_Lasso = get_prediction_responses(pred_fn_RN50_Lasso, transforms_pipeline, data_dict_day3[\"image_fps\"])\n",
    "population_pred_day3_RN50_Ridge = get_prediction_responses(pred_fn_RN50_Ridge, transforms_pipeline, data_dict_day3[\"image_fps\"])\n",
    "population_pred_day3_RN50rbst_Lasso = get_prediction_responses(pred_fn_RN50robust_Lasso, transforms_pipeline_rbst, data_dict_day3[\"image_fps\"])\n",
    "population_pred_day3_RN50rbst_Ridge = get_prediction_responses(pred_fn_RN50robust_Ridge, transforms_pipeline_rbst, data_dict_day3[\"image_fps\"])\n",
    "# ~ 3min43s per run if loading from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce838e62463943b0bfd18298cb0c3bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617b34def4654fe29dc1bf366ae8c777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110f6dbff0ee480a946a70bcec09201b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The (227, 227) setting is overwritten by the size in custom transform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6049f26f3e419a8d64c65168cfd5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x14994ab384c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    reader_close()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    self.run()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    self.run()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    reader_close()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/threading.py\", line 953, in run\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/threading.py\", line 953, in run\n",
      "    self.run()\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/threading.py\", line 953, in run\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/n/home12/binxuwang/.conda/envs/torch2/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    }
   ],
   "source": [
    "population_pred_day1_RN50_Lasso = get_prediction_responses(pred_fn_RN50_Lasso, transforms_pipeline, data_dict_day1[\"image_fps\"])\n",
    "population_pred_day1_RN50_Ridge = get_prediction_responses(pred_fn_RN50_Ridge, transforms_pipeline, data_dict_day1[\"image_fps\"])\n",
    "population_pred_day1_RN50rbst_Lasso = get_prediction_responses(pred_fn_RN50robust_Lasso, transforms_pipeline_rbst, data_dict_day1[\"image_fps\"])\n",
    "population_pred_day1_RN50rbst_Ridge = get_prediction_responses(pred_fn_RN50robust_Ridge, transforms_pipeline_rbst, data_dict_day1[\"image_fps\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
